{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5616e827",
   "metadata": {},
   "source": [
    "# Question 2:\n",
    "Consider the L2-regularized multiclass logistic regression. That is, add to the logistic regression loss a regularization term that represents the L2-norm of the parameters. More precisely, the regularization term is \n",
    "\n",
    "$ (w, b) = \\lambda \\sigma_i (||w^i||^2 + ||b^i||^2) $\n",
    "\n",
    "where ${w^i, b^i}$ are all the parameters in the logistic regression, and $\\lambda \\in R$ is the regularization hyper-parameter. Typically, $\\lambda$ is about C/n where n is the number of data points and C is some constant in `[0.01,100]` (need to tune C). Run the regularized multiclass logistic regression on MNIST, using the basic minibatch SGD, and compare its results to those of the basic minibatch SGD with non-regularized loss, in Question #1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c81bb72",
   "metadata": {},
   "source": [
    "**Import packages and load MNIST dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaa93124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d949a4",
   "metadata": {},
   "source": [
    "**Normalize the data and define a function to return a feedforward neural network model**\n",
    "\n",
    "We also applied momentum optimizer and batch size = 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adee0dc9-5f2b-4491-b293-172ed65a0761",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)\n",
    "\n",
    "# Clear any previous models from memory\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be323cf6",
   "metadata": {},
   "source": [
    "**Run the regularized multiclass logistic regression using the basic minibatch SGD**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57310f41",
   "metadata": {},
   "source": [
    "Tuning C: We use a validation split during training to evaluate performance on a portion of training data (20%). The best parameter C will be the one with the best validation performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7f53458-903b-4ced-8dac-a786c1ddb201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performing C based on validation accuracy: 0.01 with validation accuracy: 0.9608333110809326\n"
     ]
    }
   ],
   "source": [
    "n = x_train.shape[0]\n",
    "\n",
    "# Define the range of C values to try\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "best_validation_acc = 0\n",
    "best_C = None\n",
    "\n",
    "for C in C_values:\n",
    "    lambda_reg = C / n\n",
    "    regularizer = tf.keras.regularizers.L2(lambda_reg)\n",
    "    sgd = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(x_train, y_train, epochs=10, batch_size=20, verbose=0, validation_split=0.2)\n",
    "    val_acc = history.history['val_accuracy'][-1]  # take the last epoch's validation accuracy\n",
    "    # Track the best performing C\n",
    "    if val_acc > best_validation_acc:\n",
    "        best_validation_acc = val_acc\n",
    "        best_C = C\n",
    "\n",
    "\n",
    "print(f\"Best performing C based on validation accuracy: {best_C} with validation accuracy: {best_validation_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2e63dc",
   "metadata": {},
   "source": [
    "With the selected the best C, we retrained the model using all of the training data (without validation split). Finally, evaluate the model's performance on the independent test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a32c3",
   "metadata": {},
   "source": [
    "**Compare results to those of the basic minibatch SGD with non-regularized loss, in Question #1.**\n",
    "\n",
    "We observed that with L2 regularization, the accuracy (97%) on the testset is higher than the result of basic minibatch SGD with non-regularized loss, in Question 1 (96%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1508d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 498us/step - accuracy: 0.8138 - loss: 0.6057\n",
      "Epoch 2/10\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 502us/step - accuracy: 0.9582 - loss: 0.1336\n",
      "Epoch 3/10\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 538us/step - accuracy: 0.9725 - loss: 0.0889\n",
      "Epoch 4/10\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 520us/step - accuracy: 0.9787 - loss: 0.0695\n",
      "Epoch 5/10\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 499us/step - accuracy: 0.9831 - loss: 0.0512\n",
      "Epoch 6/10\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 534us/step - accuracy: 0.9863 - loss: 0.0429\n",
      "Epoch 7/10\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 495us/step - accuracy: 0.9895 - loss: 0.0325\n",
      "Epoch 8/10\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 515us/step - accuracy: 0.9901 - loss: 0.0270\n",
      "Epoch 9/10\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 504us/step - accuracy: 0.9934 - loss: 0.0201\n",
      "Epoch 10/10\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 516us/step - accuracy: 0.9929 - loss: 0.0204\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330us/step - accuracy: 0.9690 - loss: 0.1322\n",
      "Best model - Test accuracy: 0.9726999998092651, Test loss: 0.11317698657512665\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now, retrain the model with the best C on the entire training set\n",
    "n = x_train.shape[0]\n",
    "lambda_reg = best_C / n\n",
    "regularizer = tf.keras.regularizers.L2(lambda_reg)\n",
    "\n",
    "best_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizer),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizer),\n",
    "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizer),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "best_model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "best_model.fit(x_train, y_train, epochs=10, batch_size=20, verbose=1)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss, test_acc = best_model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f\"Best model - Test accuracy: {test_acc}, Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       980\n",
      "           1       0.98      0.99      0.99      1135\n",
      "           2       0.97      0.98      0.97      1032\n",
      "           3       0.95      0.97      0.96      1010\n",
      "           4       0.96      0.99      0.98       982\n",
      "           5       0.99      0.94      0.96       892\n",
      "           6       0.98      0.98      0.98       958\n",
      "           7       0.97      0.97      0.97      1028\n",
      "           8       0.96      0.97      0.97       974\n",
      "           9       0.98      0.94      0.96      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(best_model.predict(x_test), axis=1)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
