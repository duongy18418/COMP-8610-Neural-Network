{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5616e827",
   "metadata": {},
   "source": [
    "# Question 2:\n",
    "Consider the L2-regularized multiclass logistic regression. That is, add to the logistic regression loss a regularization term that represents the L2-norm of the parameters. More precisely, the regularization term is \n",
    "\n",
    "$$ (w, b) = \\lambda \\sigma_i (||w^i||^2 + ||b^i||^2) $$\n",
    "\n",
    "where ${w^i, b^i}$ are all the parameters in the logistic regression, and $\\lambda \\in R$ is the regularization hyper-parameter. Typically, $\\lambda$ is about C/n where n is the number of data points and C is some constant in `[0.01,100]` (need to tune C). Run the regularized multiclass logistic regression on MNIST, using the basic minibatch SGD, and compare its results to those of the basic minibatch SGD with non-regularized loss, in Question #1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c81bb72",
   "metadata": {},
   "source": [
    "## Import packages and load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaa93124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d949a4",
   "metadata": {},
   "source": [
    "## Normalize the data and define a function to return a feedforward neural network model\n",
    "\n",
    "We also applied momentum optimizer and batch size = 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adee0dc9-5f2b-4491-b293-172ed65a0761",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)\n",
    "\n",
    "# Clear any previous models from memory\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be323cf6",
   "metadata": {},
   "source": [
    "## Run the regularized multiclass logistic regression using the basic minibatch SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57310f41",
   "metadata": {},
   "source": [
    "Tuning C: We use a validation split during training to evaluate performance on a portion of training data (20%). The best parameter C will be the one with the best validation performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f53458-903b-4ced-8dac-a786c1ddb201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performing C based on validation accuracy: 0.1 with validation accuracy: 0.9609166383743286\n"
     ]
    }
   ],
   "source": [
    "n = x_train.shape[0]\n",
    "\n",
    "# Define the range of C values to try\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "best_validation_acc = 0\n",
    "best_C = None\n",
    "\n",
    "for C in C_values:\n",
    "    lambda_reg = C / n\n",
    "    regularizer = tf.keras.regularizers.L2(lambda_reg)\n",
    "    sgd = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(x_train, y_train, epochs=10, batch_size=20, verbose=0, validation_split=0.2)\n",
    "    val_acc = history.history['val_accuracy'][-1]  # take the last epoch's validation accuracy\n",
    "    # Track the best performing C\n",
    "    if val_acc > best_validation_acc:\n",
    "        best_validation_acc = val_acc\n",
    "        best_C = C\n",
    "\n",
    "\n",
    "print(f\"Best performing C based on validation accuracy: {best_C} with validation accuracy: {best_validation_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a32c3",
   "metadata": {},
   "source": [
    "## Compare results to those of the basic minibatch SGD with non-regularized loss, in Question #1.\n",
    "\n",
    "- With the selected the best C, we retrained the model using all of the training data (without validation split). Finally, evaluate the model's performance on the independent test set.\n",
    "- Note that we used the same setting with batch_size = 20, SGD with momentum with the first approach in Question 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1508d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2400/2400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 663us/step - accuracy: 0.7982 - loss: 0.6598 - val_accuracy: 0.9412 - val_loss: 0.1978\n",
      "Epoch 2/10\n",
      "\u001b[1m2400/2400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 608us/step - accuracy: 0.9557 - loss: 0.1434 - val_accuracy: 0.9611 - val_loss: 0.1334\n",
      "Epoch 3/10\n",
      "\u001b[1m2400/2400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 623us/step - accuracy: 0.9700 - loss: 0.0964 - val_accuracy: 0.9695 - val_loss: 0.1029\n",
      "Epoch 4/10\n",
      "\u001b[1m2400/2400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 608us/step - accuracy: 0.9795 - loss: 0.0692 - val_accuracy: 0.9662 - val_loss: 0.1109\n",
      "Epoch 5/10\n",
      "\u001b[1m2400/2400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 609us/step - accuracy: 0.9844 - loss: 0.0503 - val_accuracy: 0.9684 - val_loss: 0.1071\n",
      "Epoch 6/10\n",
      "\u001b[1m2400/2400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 606us/step - accuracy: 0.9866 - loss: 0.0410 - val_accuracy: 0.9715 - val_loss: 0.1008\n",
      "Epoch 7/10\n",
      "\u001b[1m2400/2400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 606us/step - accuracy: 0.9899 - loss: 0.0302 - val_accuracy: 0.9671 - val_loss: 0.1237\n",
      "Epoch 8/10\n",
      "\u001b[1m2400/2400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 606us/step - accuracy: 0.9908 - loss: 0.0299 - val_accuracy: 0.9729 - val_loss: 0.1056\n",
      "Epoch 9/10\n",
      "\u001b[1m2400/2400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 606us/step - accuracy: 0.9939 - loss: 0.0197 - val_accuracy: 0.9682 - val_loss: 0.1264\n",
      "Epoch 10/10\n",
      "\u001b[1m2400/2400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 626us/step - accuracy: 0.9932 - loss: 0.0217 - val_accuracy: 0.9720 - val_loss: 0.1147\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - accuracy: 0.9699 - loss: 0.1214\n",
      "Best model - Test accuracy: 0.9749000072479248, Test loss: 0.10586907714605331\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now, retrain the model with the best C on the entire training set\n",
    "n = x_train.shape[0]\n",
    "lambda_reg = best_C / n\n",
    "regularizer = tf.keras.regularizers.L2(lambda_reg)\n",
    "\n",
    "best_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizer),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizer),\n",
    "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizer),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "best_model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "best_model.fit(x_train, y_train, epochs=10, batch_size=20, validation_split=0.2)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss, test_acc = best_model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f\"Best model - Test accuracy: {test_acc}, Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       980\n",
      "           1       0.98      0.99      0.99      1135\n",
      "           2       0.98      0.98      0.98      1032\n",
      "           3       0.98      0.96      0.97      1010\n",
      "           4       0.96      0.98      0.97       982\n",
      "           5       0.98      0.98      0.98       892\n",
      "           6       0.98      0.98      0.98       958\n",
      "           7       0.98      0.97      0.98      1028\n",
      "           8       0.97      0.96      0.97       974\n",
      "           9       0.95      0.97      0.96      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(best_model.predict(x_test), axis=1)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
