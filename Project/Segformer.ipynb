{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segformer\n",
    "\n",
    "Research Paper: https://arxiv.org/abs/2105.15203\n",
    "\n",
    "Datasets: https://data.mendeley.com/datasets/8gf9vpkhgy/2\n",
    "\n",
    "Implementation adapted from:\n",
    "1. https://github.com/NVlabs/SegFormer\n",
    "2. https://debuggercafe.com/road-segmentation-using-segformer/\n",
    "3. https://www.kaggle.com/code/andrewkettle/pytorch-segformer-and-sam-on-kindey-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerConfig\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Datasets Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Load_Datasets(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[index])\n",
    "\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.transform:\n",
    "            transform = self.transform(image=image, mask=mask)\n",
    "            image = transform['image']\n",
    "            mask = transform['mask']\n",
    "\n",
    "        image = image.float()/255.0\n",
    "        mask = mask.long()\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    ToTensorV2()\n",
    "], is_check_shapes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Darwin Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "darwin_dataset = Load_Datasets(image_dir='./Datasets/Darwin/img', mask_dir='./Datasets/Darwin/mask', transform=transform)\n",
    "train, test = train_test_split(darwin_dataset, test_size=0.1)\n",
    "\n",
    "darwin_train = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "darwin_test = DataLoader(test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Shenzhen Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shenzhen_dataset = Load_Datasets(image_dir='./Datasets/Shenzhen/img', mask_dir='./Datasets/Shenzhen/mask', transform=transform)\n",
    "train, test = train_test_split(shenzhen_dataset, test_size=0.1)\n",
    "\n",
    "shenzhen_train = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "shenzhen_test = DataLoader(test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Covid-19 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_dataset = Load_Datasets(image_dir='./Datasets/Covid-19/Covid/img', mask_dir='./Datasets/Covid-19/Covid/mask', transform=transform)\n",
    "train, test = train_test_split(covid_dataset, test_size=0.1)\n",
    "\n",
    "covid_train = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "covid_test = DataLoader(test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, val_data):\n",
    "    epochs = 10\n",
    "    learning_rate = 0.0025\n",
    "\n",
    "    config = SegformerConfig(num_labels=1)\n",
    "    model = SegformerForSemanticSegmentation.from_pretrained('nvidia/mit-b0', config=config)\n",
    "\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu' # Check for device\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[20, 40], gamma=0.1)\n",
    "\n",
    "    # Train network\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "\n",
    "        # Training\n",
    "        for idx, (images, masks) in enumerate(tqdm(train_data)):\n",
    "            # Convert vars to GPU\n",
    "            images = images.float().to(device)\n",
    "            masks = masks.type(torch.LongTensor).to(device)\n",
    "            output = model(pixel_values=images, labels=masks)\n",
    "\n",
    "            loss = output.loss\n",
    "            train_loss.append(loss)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        train_loss = loss.detach().numpy()\n",
    "        print(f\"Epoch [{ep+1}/{epochs}]. Training Loss [{np.mean(train_loss)}]\")\n",
    "        scheduler.step()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Darwin Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'darwin_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m darwin_model \u001b[38;5;241m=\u001b[39m train_model(\u001b[43mdarwin_train\u001b[49m, darwin_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'darwin_train' is not defined"
     ]
    }
   ],
   "source": [
    "darwin_model = train_model(darwin_train, darwin_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Shenzhen Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 128/128 [00:41<00:00,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]. Training Loss [0.49224919080734253]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "shenzhen_model = train_model(shenzhen_train, shenzhen_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Covid-19 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 814/814 [00:51<00:00, 15.87it/s]\n",
      "100%|██████████| 91/91 [00:05<00:00, 16.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 814/814 [00:51<00:00, 15.77it/s]\n",
      "100%|██████████| 91/91 [00:05<00:00, 15.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 814/814 [00:53<00:00, 15.20it/s]\n",
      "100%|██████████| 91/91 [00:06<00:00, 14.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 814/814 [00:54<00:00, 14.83it/s]\n",
      "100%|██████████| 91/91 [00:05<00:00, 15.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 814/814 [00:54<00:00, 14.83it/s]\n",
      "100%|██████████| 91/91 [00:05<00:00, 15.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 814/814 [00:55<00:00, 14.73it/s]\n",
      "100%|██████████| 91/91 [00:06<00:00, 15.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 814/814 [00:53<00:00, 15.11it/s]\n",
      "100%|██████████| 91/91 [00:06<00:00, 15.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 814/814 [00:53<00:00, 15.15it/s]\n",
      "100%|██████████| 91/91 [00:06<00:00, 15.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 814/814 [00:52<00:00, 15.38it/s]\n",
      "100%|██████████| 91/91 [00:05<00:00, 15.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 814/814 [00:52<00:00, 15.60it/s]\n",
      "100%|██████████| 91/91 [00:05<00:00, 15.78it/s]\n"
     ]
    }
   ],
   "source": [
    "covid_model = train_model(covid_train, covid_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_data):\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu' # Check for device\n",
    "    model = model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    ious, accuracies, recalls, f1s = [], [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(val_data):\n",
    "            images = images.float().to(device)\n",
    "            masks = masks.type(torch.LongTensor).to(device)\n",
    "\n",
    "            outputs = model(pixel_values=images)\n",
    "            pred_masks = outputs.logits.argmax(dim=1)\n",
    "\n",
    "            for pred_mask, true_mask in zip(pred_masks, masks):\n",
    "                pred_mask_resized = F.interpolate(pred_mask.unsqueeze(0).unsqueeze(0).float(), size=true_mask.shape[-2:], mode='nearest').squeeze().cpu().numpy()\n",
    "                true_mask_np = true_mask.cpu().numpy()\n",
    "\n",
    "                iou = precision_score(true_mask_np.flatten(), pred_mask_resized.flatten(), average='micro')\n",
    "                accuracy = accuracy_score(true_mask_np.flatten(), pred_mask_resized.flatten(), average='micro')\n",
    "                recall = recall_score(true_mask_np.flatten(), pred_mask_resized.flatten(), average='micro')\n",
    "                f1 = f1_score(true_mask_np.flatten(), pred_mask_resized.flatten(), average='micro')\n",
    "\n",
    "                ious.append(iou)\n",
    "                accuracies.append(accuracy)\n",
    "                recalls.append(recall)\n",
    "                f1s.append(f1)\n",
    "\n",
    "    mean_iou = np.mean(ious)\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    mean_recall = np.mean(recalls)\n",
    "    mean_f1 = np.mean(f1s)\n",
    "\n",
    "    return mean_iou, mean_accuracy, mean_recall, mean_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Darwin Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:03<00:00,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics - IoU: 0.7431587085389254, Precision: 0.7431587085389254, Recall: 0.7431587085389254, F1 Score: 0.7431587085389254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_iou, mean_accuracy, mean_recall, mean_f1 = evaluate_model(darwin_model, darwin_test)\n",
    "\n",
    "print(f\"Validation Metrics:\n",
    "        IoU: {mean_iou}, \n",
    "        Accuracy: {mean_accuracy}, \n",
    "        Recall: {mean_recall}, \n",
    "        F1 Score: {mean_f1}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Shenzhen Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:03<00:00,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics - IoU: 0.7431587085389254, Precision: 0.7431587085389254, Recall: 0.7431587085389254, F1 Score: 0.7431587085389254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_iou, mean_accuracy, mean_recall, mean_f1 = evaluate_model(shenzhen_model, shenzhen_test)\n",
    "\n",
    "print(f\"Validation Metrics:\n",
    "        IoU: {mean_iou}, \n",
    "        Accuracy: {mean_accuracy}, \n",
    "        Recall: {mean_recall}, \n",
    "        F1 Score: {mean_f1}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Covid-19 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:03<00:00,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics - IoU: 0.7431587085389254, Precision: 0.7431587085389254, Recall: 0.7431587085389254, F1 Score: 0.7431587085389254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_iou, mean_accuracy, mean_recall, mean_f1 = evaluate_model(covid_model, covid_test)\n",
    "\n",
    "print(f\"Validation Metrics:\n",
    "        IoU: {mean_iou}, \n",
    "        Accuracy: {mean_accuracy}, \n",
    "        Recall: {mean_recall}, \n",
    "        F1 Score: {mean_f1}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
