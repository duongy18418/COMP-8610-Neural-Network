{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segformer\n",
    "\n",
    "Research Paper: https://arxiv.org/abs/2105.15203\n",
    "\n",
    "Datasets: https://data.mendeley.com/datasets/8gf9vpkhgy/2\n",
    "\n",
    "Implementation adapted from:\n",
    "1. https://github.com/NVlabs/SegFormer\n",
    "2. https://debuggercafe.com/road-segmentation-using-segformer/\n",
    "3. https://www.kaggle.com/code/andrewkettle/pytorch-segformer-and-sam-on-kindey-1\n",
    "4. https://medium.com/geekculture/semantic-segmentation-with-segformer-2501543d2be4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor, SegformerConfig\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, jaccard_score, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Datasets Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Process_Datasets(Dataset):\n",
    "    def __init__(self, root_dir, feature_extractor, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.image_path = os.path.join(self.root_dir, \"img\")\n",
    "        self.mask_path = os.path.join(self.root_dir, \"mask\")\n",
    "\n",
    "        img_files = []\n",
    "        for root, dirs, files in os.walk(self.image_path):\n",
    "            img_files.extend(files)\n",
    "        self.images = sorted(img_files)\n",
    "\n",
    "        mask_files = []\n",
    "        for root, dirs, files in os.walk(self.mask_path):\n",
    "            mask_files.extend(files)\n",
    "        self.masks = sorted(mask_files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.image_path, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_path, self.masks[index])\n",
    "\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.transforms:\n",
    "            transform = self.transforms(image=image, mask=mask)\n",
    "            encoded = self.feature_extractor(transform['image'], transform['mask'], return_tensors=\"pt\")\n",
    "        else:\n",
    "            encoded = self.feature_extractor(image, mask, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded.items():\n",
    "            encoded[k].squeeze_()\n",
    "\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(root_dir):\n",
    "    batch_size=4\n",
    "    feature_extractor = SegformerFeatureExtractor(align=False, reduce_zero_label=False)    \n",
    "    transform = A.Compose([\n",
    "        A.Resize(128, 128),\n",
    "        ToTensorV2()\n",
    "    ], is_check_shapes=False)\n",
    "\n",
    "    dataset = Process_Datasets(root_dir=root_dir, feature_extractor=feature_extractor, transforms=transform)\n",
    "    train, val = train_test_split(dataset, test_size=0.2, random_state=5)\n",
    "\n",
    "    train_dataset = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Darwin Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1221, 306)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "darwin_train, darwin_val = load_datasets(root_dir=\"./Datasets/Darwin\")\n",
    "len(darwin_train), len(darwin_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Shenzhen Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113, 29)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shenzhen_train, shenzhen_val = load_datasets(root_dir=\"./Datasets/Shenzhen\")\n",
    "len(shenzhen_train), len(shenzhen_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Covid-19 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(723, 181)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_train, covid_val = load_datasets(root_dir=\"./Datasets/COVID-19/COVID\")\n",
    "len(covid_train), len(covid_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "def train_model(train_data, val_data):\n",
    "    epochs = 6\n",
    "    learning_rate = 0.0025\n",
    "\n",
    "    config = SegformerConfig(num_labels=1)\n",
    "    model = SegformerForSemanticSegmentation.from_pretrained('nvidia/mit-b0', config=config)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    val_metrics = []\n",
    "\n",
    "    # Train network\n",
    "    for ep in range(epochs):\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "\n",
    "        model.train()\n",
    "        for index, batch in enumerate(tqdm(train_data)):\n",
    "            image = batch[\"pixel_values\"]\n",
    "            mask = batch[\"labels\"]\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(pixel_values=image, labels=mask)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for index, batch in enumerate(tqdm(val_data)):\n",
    "                image = batch[\"pixel_values\"]\n",
    "                mask = batch[\"labels\"]\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(pixel_values=image, labels=mask)\n",
    "                logits = F.interpolate(outputs.logits, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "                prediction = logits.argmax(dim=1)\n",
    "\n",
    "                for pred, true in zip(prediction, mask):\n",
    "                    pred_mask = pred.cpu().numpy()\n",
    "                    true_mask = true.cpu().numpy()\n",
    "\n",
    "                    iou = jaccard_score(true_mask.flatten(), pred_mask.flatten(), average='weighted')\n",
    "                    accuracy = accuracy_score(true_mask.flatten(), pred_mask.flatten())\n",
    "                    precision = precision_score(true_mask.flatten(), pred_mask.flatten(), average='weighted')\n",
    "                    recall = recall_score(true_mask.flatten(), pred_mask.flatten(), average='weighted')\n",
    "                    f1 = f1_score(true_mask.flatten(), pred_mask.flatten(), average='weighted')\n",
    "\n",
    "                    val_metrics.append([iou, accuracy, precision, recall, f1])\n",
    "\n",
    "                loss = outputs.loss\n",
    "                val_loss.append(loss.item())\n",
    "                optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{ep+1}/{epochs}]. Training Loss [{np.mean(train_loss)}]. Validation Loss [{np.mean(val_loss)}]\")\n",
    "\n",
    "    metrics = pd.DataFrame(val_metrics, columns=[\"IoU\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Darwin Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1221/1221 [05:52<00:00,  3.47it/s]\n",
      "100%|██████████| 306/306 [04:16<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6]. Training Loss [0.4805369806475175]. Validation Loss [0.47518783427920996]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1221/1221 [05:49<00:00,  3.49it/s]\n",
      "100%|██████████| 306/306 [04:17<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/6]. Training Loss [0.4808707416643382]. Validation Loss [0.4776922834464927]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1221/1221 [05:50<00:00,  3.49it/s]\n",
      "100%|██████████| 306/306 [04:15<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/6]. Training Loss [0.4806338767357091]. Validation Loss [0.47651069382436917]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1221/1221 [05:49<00:00,  3.49it/s]\n",
      "100%|██████████| 306/306 [04:15<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/6]. Training Loss [0.4798344814757848]. Validation Loss [0.4787420781414493]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1221/1221 [05:49<00:00,  3.49it/s]\n",
      "100%|██████████| 306/306 [04:15<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/6]. Training Loss [0.480380810315154]. Validation Loss [0.4792502614018185]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1221/1221 [05:48<00:00,  3.50it/s]\n",
      "100%|██████████| 306/306 [04:15<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/6]. Training Loss [0.48025338911893034]. Validation Loss [0.4773622329523361]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "darwin_model, darwin_metrics = train_model(darwin_train, darwin_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Shenzhen Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 113/113 [00:33<00:00,  3.38it/s]\n",
      "100%|██████████| 29/29 [00:24<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6]. Training Loss [0.5650780723158237]. Validation Loss [0.5577489343182794]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:32<00:00,  3.45it/s]\n",
      "100%|██████████| 29/29 [00:24<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/6]. Training Loss [0.5646604542183665]. Validation Loss [0.5639664736287348]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:32<00:00,  3.45it/s]\n",
      "100%|██████████| 29/29 [00:24<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/6]. Training Loss [0.5650270259485836]. Validation Loss [0.5603276532271813]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:32<00:00,  3.45it/s]\n",
      "100%|██████████| 29/29 [00:23<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/6]. Training Loss [0.5638870449192757]. Validation Loss [0.5590729507906683]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:32<00:00,  3.45it/s]\n",
      "100%|██████████| 29/29 [00:23<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/6]. Training Loss [0.5640107298319319]. Validation Loss [0.5601615679675135]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:32<00:00,  3.45it/s]\n",
      "100%|██████████| 29/29 [00:23<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/6]. Training Loss [0.5639024250275266]. Validation Loss [0.5611148641027254]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "shenzhen_model, shenzhen_metrics = train_model(shenzhen_train, shenzhen_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Covid-19 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 723/723 [03:30<00:00,  3.44it/s]\n",
      "100%|██████████| 181/181 [03:04<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6]. Training Loss [0.5070195704245138]. Validation Loss [0.5091175147183034]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 723/723 [03:28<00:00,  3.47it/s]\n",
      "100%|██████████| 181/181 [02:58<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/6]. Training Loss [0.507060460365]. Validation Loss [0.5010465138195628]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 723/723 [03:27<00:00,  3.48it/s]\n",
      "100%|██████████| 181/181 [02:59<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/6]. Training Loss [0.507034755676449]. Validation Loss [0.5120224997154257]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 723/723 [03:27<00:00,  3.49it/s]\n",
      "100%|██████████| 181/181 [03:07<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/6]. Training Loss [0.5069860616907539]. Validation Loss [0.5097301406425666]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 723/723 [03:28<00:00,  3.47it/s]\n",
      "100%|██████████| 181/181 [03:02<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/6]. Training Loss [0.5070240113827532]. Validation Loss [0.5061063389422485]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 723/723 [03:32<00:00,  3.40it/s]\n",
      "100%|██████████| 181/181 [03:04<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/6]. Training Loss [0.5068797120002625]. Validation Loss [0.5008786123101883]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "covid_model, covid_metrics = train_model(covid_train, covid_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "darwin_metrics.to_csv(\"./results/darwin_segformer.csv\", index=False)\n",
    "shenzhen_metrics.to_csv(\"./results/shenzhen_segformer.csv\", index=False)\n",
    "covid_metrics.to_csv(\"./results/covid_segformer.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(metric):\n",
    "    avg_iou = metric['IoU'].mean()\n",
    "    avg_accur = metric['Accuracy'].mean()\n",
    "    avg_prec = metric['Precision'].mean()\n",
    "    avg_recall = metric['Recall'].mean()\n",
    "    avg_f1 = metric['F1'].mean()\n",
    "\n",
    "    print(f\"Validation Metrics: IoU: {avg_iou}, Accuracy: {avg_accur}, Precision: {avg_prec}, Recall: {avg_recall}, F1 Score: {avg_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Darwin Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics: IoU: 0.4574702863784205, Accuracy: 0.6720253902254245, Precision: 0.4574702863784205, Recall: 0.6720253902254245, F1 Score: 0.542628702175488\n"
     ]
    }
   ],
   "source": [
    "evaluation(darwin_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Shenzhen Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics: IoU: 0.561202498250886, Accuracy: 0.7467041015625, Precision: 0.561202498250886, Recall: 0.7467041015625, F1 Score: 0.6397997846948161\n"
     ]
    }
   ],
   "source": [
    "evaluation(shenzhen_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Covid-19 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics: IoU: 0.5670235187731149, Accuracy: 0.7491341269477296, Precision: 0.5670235187731149, Recall: 0.7491341269477296, F1 Score: 0.6439514189280622\n"
     ]
    }
   ],
   "source": [
    "evaluation(covid_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
