{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segformer\n",
    "\n",
    "Research Paper: https://arxiv.org/abs/2105.15203\n",
    "\n",
    "Datasets: https://data.mendeley.com/datasets/8gf9vpkhgy/2\n",
    "\n",
    "Implementation adapted from:\n",
    "1. https://github.com/NVlabs/SegFormer\n",
    "2. https://debuggercafe.com/road-segmentation-using-segformer/\n",
    "3. https://www.kaggle.com/code/andrewkettle/pytorch-segformer-and-sam-on-kindey-1\n",
    "4. https://medium.com/geekculture/semantic-segmentation-with-segformer-2501543d2be4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\duong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor, SegformerConfig\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, jaccard_score\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Datasets Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Process_Datasets(Dataset):\n",
    "    def __init__(self, root_dir, feature_extractor, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.image_path = os.path.join(self.root_dir, \"img\")\n",
    "        self.mask_path = os.path.join(self.root_dir, \"mask\")\n",
    "\n",
    "        img_files = []\n",
    "        for root, dirs, files in os.walk(self.image_path):\n",
    "            img_files.extend(files)\n",
    "        self.images = sorted(img_files)\n",
    "\n",
    "        mask_files = []\n",
    "        for root, dirs, files in os.walk(self.mask_path):\n",
    "            mask_files.extend(files)\n",
    "        self.masks = sorted(mask_files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.image_path, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_path, self.masks[index])\n",
    "\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.transforms:\n",
    "            transform = self.transforms(image=image, mask=mask)\n",
    "            encoded = self.feature_extractor(transform['image'], transform['mask'], return_tensors=\"pt\")\n",
    "        else:\n",
    "            encoded = self.feature_extractor(image, mask, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded.items():\n",
    "            encoded[k].squeeze_()\n",
    "\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(root_dir):\n",
    "    batch_size=4\n",
    "    feature_extractor = SegformerFeatureExtractor(align=False, reduce_zero_label=False)    \n",
    "    transform = A.Compose([\n",
    "        A.Resize(128, 128),\n",
    "        ToTensorV2()\n",
    "    ], is_check_shapes=False)\n",
    "\n",
    "    dataset = Process_Datasets(root_dir=root_dir, feature_extractor=feature_extractor, transforms=transform)\n",
    "    train, val = train_test_split(dataset, test_size=0.1, random_state=5)\n",
    "    val, test = train_test_split(val, test_size=0.1, random_state=5)\n",
    "\n",
    "    train_dataset = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "    test_dataset = DataLoader(test, shuffle=True)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Darwin Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\duong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\segformer\\feature_extraction_segformer.py:28: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\duong\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\deprecation.py:165: UserWarning: The following named arguments are not valid for `SegformerFeatureExtractor.__init__` and were ignored: 'align', 'reduce_zero_label'\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1374, 138, 62)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "darwin_train, darwin_val, darwin_test = load_datasets(root_dir=\"./Datasets/Darwin\")\n",
    "len(darwin_train), len(darwin_val), len(darwin_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Shenzhen Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 13, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shenzhen_train, shenzhen_val, shenzhen_test = load_datasets(root_dir=\"./Datasets/Shenzhen\")\n",
    "len(shenzhen_train), len(shenzhen_val), len(shenzhen_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Covid-19 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(814, 37)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_train, covid_val, covid_test = load_datasets(root_dir=\"./Datasets/COVID-19/COVID\")\n",
    "len(covid_train), len(covid_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, val_data):\n",
    "    epochs = 10\n",
    "    learning_rate = 0.0025\n",
    "\n",
    "    config = SegformerConfig(num_labels=1)\n",
    "    model = SegformerForSemanticSegmentation.from_pretrained('nvidia/mit-b0', config=config)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train network\n",
    "    for ep in range(epochs):\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "\n",
    "        model.train()\n",
    "        for index, batch in enumerate(tqdm(train_data)):\n",
    "            pixel_values = batch[\"pixel_values\"]\n",
    "            labels = batch[\"labels\"]\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for index, batch in enumerate(tqdm(val_data)):\n",
    "                pixel_values = batch[\"pixel_values\"]\n",
    "                labels = batch[\"labels\"]\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "                val_loss.append(loss.item())\n",
    "                optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{ep+1}/{epochs}]. Training Loss [{np.mean(train_loss)}]. Validation Loss [{np.mean(val_loss)}]\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Darwin Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1374/1374 [06:39<00:00,  3.44it/s]\n",
      "100%|██████████| 138/138 [00:34<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]. Training Loss [0.43524463944885405]. Validation Loss [0.4398825751698535]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374/1374 [06:32<00:00,  3.50it/s]\n",
      "100%|██████████| 138/138 [00:34<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]. Training Loss [0.43424851978753504]. Validation Loss [0.43674293311609735]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374/1374 [06:33<00:00,  3.50it/s]\n",
      "100%|██████████| 138/138 [00:34<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]. Training Loss [0.4343823699234528]. Validation Loss [0.4375596547472304]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374/1374 [06:33<00:00,  3.49it/s]\n",
      "100%|██████████| 138/138 [00:34<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]. Training Loss [0.4343797474600689]. Validation Loss [0.4378686169351357]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374/1374 [06:33<00:00,  3.50it/s]\n",
      "100%|██████████| 138/138 [00:34<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]. Training Loss [0.4346866974664047]. Validation Loss [0.44085417169591656]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374/1374 [06:33<00:00,  3.49it/s]\n",
      "100%|██████████| 138/138 [00:34<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]. Training Loss [0.4342090359399586]. Validation Loss [0.4405207497918088]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374/1374 [06:33<00:00,  3.49it/s]\n",
      "100%|██████████| 138/138 [00:34<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]. Training Loss [0.43555243776398106]. Validation Loss [0.43768960107927735]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374/1374 [06:33<00:00,  3.50it/s]\n",
      "100%|██████████| 138/138 [00:34<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]. Training Loss [0.43387104868563503]. Validation Loss [0.43755757074425183]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374/1374 [06:32<00:00,  3.50it/s]\n",
      "100%|██████████| 138/138 [00:34<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]. Training Loss [0.43431192622391246]. Validation Loss [0.43714567371036694]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374/1374 [06:33<00:00,  3.49it/s]\n",
      "100%|██████████| 138/138 [00:34<00:00,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]. Training Loss [0.4347247025743669]. Validation Loss [0.4380907317002614]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "darwin_model = train_model(darwin_train, darwin_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Shenzhen Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 128/128 [00:36<00:00,  3.49it/s]\n",
      "100%|██████████| 13/13 [00:03<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]. Training Loss [0.5627518070396036]. Validation Loss [0.558212005175077]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:36<00:00,  3.52it/s]\n",
      "100%|██████████| 13/13 [00:03<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]. Training Loss [0.5629258088301867]. Validation Loss [0.5568674848629878]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:36<00:00,  3.53it/s]\n",
      "100%|██████████| 13/13 [00:03<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]. Training Loss [0.5634880259167403]. Validation Loss [0.5636127522358527]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:36<00:00,  3.52it/s]\n",
      "100%|██████████| 13/13 [00:03<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]. Training Loss [0.5640577164012939]. Validation Loss [0.5646736117509695]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:36<00:00,  3.52it/s]\n",
      "100%|██████████| 13/13 [00:03<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]. Training Loss [0.5626006373204291]. Validation Loss [0.5647830917285039]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:36<00:00,  3.52it/s]\n",
      "100%|██████████| 13/13 [00:03<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]. Training Loss [0.563137001125142]. Validation Loss [0.560822090277305]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:36<00:00,  3.53it/s]\n",
      "100%|██████████| 13/13 [00:03<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]. Training Loss [0.5628131642006338]. Validation Loss [0.565063999249385]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:36<00:00,  3.52it/s]\n",
      "100%|██████████| 13/13 [00:03<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]. Training Loss [0.562933347420767]. Validation Loss [0.5645095751835749]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:36<00:00,  3.52it/s]\n",
      "100%|██████████| 13/13 [00:03<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]. Training Loss [0.5627028229646385]. Validation Loss [0.5661531274135296]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:36<00:00,  3.52it/s]\n",
      "100%|██████████| 13/13 [00:03<00:00,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]. Training Loss [0.5627696823794395]. Validation Loss [0.5640419813302847]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "shenzhen_model = train_model(shenzhen_train, shenzhen_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Covid-19 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 814/814 [03:52<00:00,  3.50it/s]\n",
      "100%|██████████| 82/82 [00:20<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]. Training Loss [0.5540234648258738]. Validation Loss [0.5446027161144629]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 814/814 [03:52<00:00,  3.51it/s]\n",
      "100%|██████████| 82/82 [00:20<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]. Training Loss [0.5542196601409584]. Validation Loss [0.5470831466884147]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 814/814 [03:53<00:00,  3.49it/s]\n",
      "100%|██████████| 82/82 [00:20<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]. Training Loss [0.5541503212302558]. Validation Loss [0.5486820943471862]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 814/814 [03:52<00:00,  3.50it/s]\n",
      "100%|██████████| 82/82 [00:20<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]. Training Loss [0.553922365170146]. Validation Loss [0.5494555641965169]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 814/814 [03:52<00:00,  3.50it/s]\n",
      "100%|██████████| 82/82 [00:20<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]. Training Loss [0.5542508487472956]. Validation Loss [0.5428105360124169]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 814/814 [03:52<00:00,  3.50it/s]\n",
      "100%|██████████| 82/82 [00:20<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]. Training Loss [0.5541196159780465]. Validation Loss [0.5514216495723259]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 814/814 [03:53<00:00,  3.49it/s]\n",
      "100%|██████████| 82/82 [00:20<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]. Training Loss [0.5540672229461061]. Validation Loss [0.5518807092817818]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 814/814 [03:53<00:00,  3.49it/s]\n",
      "100%|██████████| 82/82 [00:20<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]. Training Loss [0.55410531503709]. Validation Loss [0.549522464231747]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 814/814 [03:52<00:00,  3.49it/s]\n",
      "100%|██████████| 82/82 [00:20<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]. Training Loss [0.5542291139941251]. Validation Loss [0.5509657812554661]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 814/814 [03:52<00:00,  3.50it/s]\n",
      "100%|██████████| 82/82 [00:20<00:00,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]. Training Loss [0.5541447640490473]. Validation Loss [0.5512824116683588]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "covid_model = train_model(covid_train, covid_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "def evaluate_model(model, test_data):\n",
    "    batch = next(iter(test_data))\n",
    "    ious, accuracies, recalls, f1s = [], [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for index, batch in enumerate(tqdm(test_data)):\n",
    "            image = batch[\"pixel_values\"]\n",
    "            mask = batch[\"labels\"]\n",
    "\n",
    "            outputs = model(pixel_values=image, labels=mask)\n",
    "            logits = F.interpolate(outputs.logits, size=mask.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "            prediction = logits.argmax(dim=1)\n",
    "\n",
    "            for pred, true in zip(prediction, mask):\n",
    "                pred_mask = pred.cpu().numpy()\n",
    "                true_mask = true.cpu().numpy()\n",
    "\n",
    "                iou = jaccard_score(true_mask.flatten(), pred_mask.flatten(), average='weighted')\n",
    "                accuracy = accuracy_score(true_mask.flatten(), pred_mask.flatten())\n",
    "                recall = recall_score(true_mask.flatten(), pred_mask.flatten(), average='weighted')\n",
    "                f1 = f1_score(true_mask.flatten(), pred_mask.flatten(), average='weighted')\n",
    "\n",
    "                ious.append(iou)\n",
    "                accuracies.append(accuracy)\n",
    "                recalls.append(recall)\n",
    "                f1s.append(f1)\n",
    "\n",
    "    mean_iou = np.mean(ious)\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    mean_recall = np.mean(recalls)\n",
    "    mean_f1 = np.mean(f1s)\n",
    "\n",
    "    return mean_iou, mean_accuracy, mean_recall, mean_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Darwin Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:15<00:00,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics: IoU: 0.4488634690642357, Accuracy: 0.6655332503780242, Recall: 0.6655332503780242, F1 Score: 0.5344151998205954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "darwin_iou, darwin_accuracy, darwin_recall, darwin_f1 = evaluate_model(darwin_model, darwin_test)\n",
    "\n",
    "print(f\"Validation Metrics: IoU: {darwin_iou}, Accuracy: {darwin_accuracy}, Recall: {darwin_recall}, F1 Score: {darwin_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Shenzhen Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics: IoU: 0.5149720447758833, Accuracy: 0.7153523763020834, Recall: 0.7153523763020834, F1 Score: 0.5979566650943469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "shenzhen_iou, shenzhen_accuracy, shenzhen_recall, shenzhen_f1 = evaluate_model(shenzhen_model, shenzhen_test)\n",
    "\n",
    "print(f\"Validation Metrics: IoU: {shenzhen_iou}, Accuracy: {shenzhen_accuracy}, Recall: {shenzhen_recall}, F1 Score: {shenzhen_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Covid-19 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:10<00:00,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics: IoU: 0.561079980855858, Accuracy: 0.7448812948690878, Recall: 0.7448812948690878, F1 Score: 0.6384047276710346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "covid_iou, covid_accuracy, covid_recall, covid_f1 = evaluate_model(covid_model, covid_test)\n",
    "\n",
    "print(f\"Validation Metrics: IoU: {covid_iou}, Accuracy: {covid_accuracy}, Recall: {covid_recall}, F1 Score: {covid_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-------------------+----------------+-------------+\n",
      "| Datasets   |   IoU Score |    Accuracy Score |   Recall Score |   F-1 score |\n",
      "+============+=============+===================+================+=============+\n",
      "| Darwin     |    0.448863 |          0.665533 |       0.665533 |    0.534415 |\n",
      "+------------+-------------+-------------------+----------------+-------------+\n",
      "| Zhenshen   |    0.514972 |          0.715352 |       0.715352 |    0.597957 |\n",
      "+------------+-------------+-------------------+----------------+-------------+\n",
      "| Covid-19   |    0.56108  |          0.744881 |       0.744881 |    0.638405 |\n",
      "+------------+-------------+-------------------+----------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "results_table = [\n",
    "    [\"Darwin\", darwin_iou, darwin_accuracy, darwin_recall, darwin_f1],\n",
    "    [\"Zhenshen\", shenzhen_iou, shenzhen_accuracy, shenzhen_recall, shenzhen_f1],\n",
    "    [\"Covid-19\", covid_iou, covid_accuracy, covid_recall, covid_f1]\n",
    "]\n",
    "\n",
    "head = [\"Datasets\", \"IoU Score\", \" Accuracy Score\", \"Recall Score\", \"F-1 score\"]\n",
    "\n",
    "print(tabulate(results_table, headers=head, tablefmt=\"grid\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
