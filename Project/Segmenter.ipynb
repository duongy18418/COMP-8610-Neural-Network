{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmenter\n",
    "\n",
    "Implementation is adopted from https://github.com/rstrudel/segmenter/\n",
    "\n",
    "Author Paper: https://arxiv.org/abs/2105.05633"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install einops timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Adapted from 2020 Ross Wightman\n",
    "https://github.com/rwightman/pytorch-image-models\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import DropPath\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout, out_dim=None):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        if out_dim is None:\n",
    "            out_dim = dim\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    @property\n",
    "    def unwrapped(self):\n",
    "        return self\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        head_dim = dim // heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.attn = None\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "\n",
    "    @property\n",
    "    def unwrapped(self):\n",
    "        return self\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, N, C = x.shape\n",
    "        qkv = (\n",
    "            self.qkv(x)\n",
    "            .reshape(B, N, 3, self.heads, C // self.heads)\n",
    "            .permute(2, 0, 3, 1, 4)\n",
    "        )\n",
    "        q, k, v = (\n",
    "            qkv[0],\n",
    "            qkv[1],\n",
    "            qkv[2],\n",
    "        )\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_dim, dropout, drop_path):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, heads, dropout)\n",
    "        self.mlp = FeedForward(dim, mlp_dim, dropout)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x), mask)\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from timm.models.layers import trunc_normal_\n",
    "\n",
    "##### outer folder util\n",
    "import os\n",
    "import torch\n",
    "\"\"\"\n",
    "GPU wrappers\n",
    "\"\"\"\n",
    "\n",
    "use_gpu = False\n",
    "gpu_id = 0\n",
    "device = None\n",
    "\n",
    "distributed = False\n",
    "dist_rank = 0\n",
    "world_size = 1\n",
    "\n",
    "\n",
    "def set_gpu_mode(mode):\n",
    "    global use_gpu\n",
    "    global device\n",
    "    global gpu_id\n",
    "    global distributed\n",
    "    global dist_rank\n",
    "    global world_size\n",
    "    gpu_id = int(os.environ.get(\"SLURM_LOCALID\", 0))\n",
    "    dist_rank = int(os.environ.get(\"SLURM_PROCID\", 0))\n",
    "    world_size = int(os.environ.get(\"SLURM_NTASKS\", 1))\n",
    "\n",
    "    distributed = world_size > 1\n",
    "    use_gpu = mode\n",
    "    device = torch.device(f\"cuda:{gpu_id}\" if use_gpu else \"cpu\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        trunc_normal_(m.weight, std=0.02)\n",
    "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "        nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "\n",
    "def resize_pos_embed(posemb, grid_old_shape, grid_new_shape, num_extra_tokens):\n",
    "    # Rescale the grid of position embeddings when loading from state_dict. Adapted from\n",
    "    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224\n",
    "    posemb_tok, posemb_grid = (\n",
    "        posemb[:, :num_extra_tokens],\n",
    "        posemb[0, num_extra_tokens:],\n",
    "    )\n",
    "    if grid_old_shape is None:\n",
    "        gs_old_h = int(math.sqrt(len(posemb_grid)))\n",
    "        gs_old_w = gs_old_h\n",
    "    else:\n",
    "        gs_old_h, gs_old_w = grid_old_shape\n",
    "\n",
    "    gs_h, gs_w = grid_new_shape\n",
    "    posemb_grid = posemb_grid.reshape(1, gs_old_h, gs_old_w, -1).permute(0, 3, 1, 2)\n",
    "    posemb_grid = F.interpolate(posemb_grid, size=(gs_h, gs_w), mode=\"bilinear\")\n",
    "    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, gs_h * gs_w, -1)\n",
    "    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)\n",
    "    return posemb\n",
    "\n",
    "\n",
    "def checkpoint_filter_fn(state_dict, model):\n",
    "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
    "    out_dict = {}\n",
    "    if \"model\" in state_dict:\n",
    "        # For deit models\n",
    "        state_dict = state_dict[\"model\"]\n",
    "    num_extra_tokens = 1 + (\"dist_token\" in state_dict.keys())\n",
    "    patch_size = model.patch_size\n",
    "    image_size = model.patch_embed.image_size\n",
    "    for k, v in state_dict.items():\n",
    "        if k == \"pos_embed\" and v.shape != model.pos_embed.shape:\n",
    "            # To resize pos embedding when using model at different size from pretrained weights\n",
    "            v = resize_pos_embed(\n",
    "                v,\n",
    "                None,\n",
    "                (image_size[0] // patch_size, image_size[1] // patch_size),\n",
    "                num_extra_tokens,\n",
    "            )\n",
    "        out_dict[k] = v\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "def padding(im, patch_size, fill_value=0):\n",
    "    # make the image sizes divisible by patch_size\n",
    "    H, W = im.size(2), im.size(3)\n",
    "    pad_h, pad_w = 0, 0\n",
    "    if H % patch_size > 0:\n",
    "        pad_h = patch_size - (H % patch_size)\n",
    "    if W % patch_size > 0:\n",
    "        pad_w = patch_size - (W % patch_size)\n",
    "    im_padded = im\n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        im_padded = F.pad(im, (0, pad_w, 0, pad_h), value=fill_value)\n",
    "    return im_padded\n",
    "\n",
    "\n",
    "def unpadding(y, target_size):\n",
    "    H, W = target_size\n",
    "    H_pad, W_pad = y.size(2), y.size(3)\n",
    "    # crop predictions on extra pixels coming from padding\n",
    "    extra_h = H_pad - H\n",
    "    extra_w = W_pad - W\n",
    "    if extra_h > 0:\n",
    "        y = y[:, :, :-extra_h]\n",
    "    if extra_w > 0:\n",
    "        y = y[:, :, :, :-extra_w]\n",
    "    return y\n",
    "\n",
    "\n",
    "def resize(im, smaller_size):\n",
    "    h, w = im.shape[2:]\n",
    "    if h < w:\n",
    "        ratio = w / h\n",
    "        h_res, w_res = smaller_size, ratio * smaller_size\n",
    "    else:\n",
    "        ratio = h / w\n",
    "        h_res, w_res = ratio * smaller_size, smaller_size\n",
    "    if min(h, w) < smaller_size:\n",
    "        im_res = F.interpolate(im, (int(h_res), int(w_res)), mode=\"bilinear\")\n",
    "    else:\n",
    "        im_res = im\n",
    "    return im_res\n",
    "\n",
    "\n",
    "def sliding_window(im, flip, window_size, window_stride):\n",
    "    B, C, H, W = im.shape\n",
    "    ws = window_size\n",
    "\n",
    "    windows = {\"crop\": [], \"anchors\": []}\n",
    "    h_anchors = torch.arange(0, H, window_stride)\n",
    "    w_anchors = torch.arange(0, W, window_stride)\n",
    "    h_anchors = [h.item() for h in h_anchors if h < H - ws] + [H - ws]\n",
    "    w_anchors = [w.item() for w in w_anchors if w < W - ws] + [W - ws]\n",
    "    for ha in h_anchors:\n",
    "        for wa in w_anchors:\n",
    "            window = im[:, :, ha : ha + ws, wa : wa + ws]\n",
    "            windows[\"crop\"].append(window)\n",
    "            windows[\"anchors\"].append((ha, wa))\n",
    "    windows[\"flip\"] = flip\n",
    "    windows[\"shape\"] = (H, W)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def merge_windows(windows, window_size, ori_shape):\n",
    "    ws = window_size\n",
    "    im_windows = windows[\"seg_maps\"]\n",
    "    anchors = windows[\"anchors\"]\n",
    "    C = im_windows[0].shape[0]\n",
    "    H, W = windows[\"shape\"]\n",
    "    flip = windows[\"flip\"]\n",
    "\n",
    "    logit = torch.zeros((C, H, W), device=im_windows.device)\n",
    "    count = torch.zeros((1, H, W), device=im_windows.device)\n",
    "    for window, (ha, wa) in zip(im_windows, anchors):\n",
    "        logit[:, ha : ha + ws, wa : wa + ws] += window\n",
    "        count[:, ha : ha + ws, wa : wa + ws] += 1\n",
    "    logit = logit / count\n",
    "    logit = F.interpolate(\n",
    "        logit.unsqueeze(0),\n",
    "        ori_shape,\n",
    "        mode=\"bilinear\",\n",
    "    )[0]\n",
    "    if flip:\n",
    "        logit = torch.flip(logit, (2,))\n",
    "    result = F.softmax(logit, 0)\n",
    "    return result\n",
    "\n",
    "\n",
    "def inference(\n",
    "    model,\n",
    "    ims,\n",
    "    ims_metas,\n",
    "    ori_shape,\n",
    "    window_size,\n",
    "    window_stride,\n",
    "    batch_size,\n",
    "):\n",
    "    C = model.n_cls\n",
    "    seg_map = torch.zeros((C, ori_shape[0], ori_shape[1]), device=device)\n",
    "    for im, im_metas in zip(ims, ims_metas):\n",
    "        im = im.to(device)\n",
    "        im = resize(im, window_size)\n",
    "        flip = im_metas[\"flip\"]\n",
    "        windows = sliding_window(im, flip, window_size, window_stride)\n",
    "        crops = torch.stack(windows.pop(\"crop\"))[:, 0]\n",
    "        B = len(crops)\n",
    "        WB = batch_size\n",
    "        seg_maps = torch.zeros((B, C, window_size, window_size), device=im.device)\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, B, WB):\n",
    "                seg_maps[i : i + WB] = model.forward(crops[i : i + WB])\n",
    "        windows[\"seg_maps\"] = seg_maps\n",
    "        im_seg_map = merge_windows(windows, window_size, ori_shape)\n",
    "        seg_map += im_seg_map\n",
    "    seg_map /= len(ims)\n",
    "    return seg_map\n",
    "\n",
    "\n",
    "def num_params(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    n_params = sum([torch.prod(torch.tensor(p.size())) for p in model_parameters])\n",
    "    return n_params.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "from timm.models.layers import trunc_normal_\n",
    "\n",
    "class DecoderLinear(nn.Module):\n",
    "    def __init__(self, n_cls, patch_size, d_encoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_encoder = d_encoder\n",
    "        self.patch_size = patch_size\n",
    "        self.n_cls = n_cls\n",
    "\n",
    "        self.head = nn.Linear(self.d_encoder, n_cls)\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return set()\n",
    "\n",
    "    def forward(self, x, im_size):\n",
    "        H, W = im_size\n",
    "        GS = H // self.patch_size\n",
    "        x = self.head(x)\n",
    "        x = rearrange(x, \"b (h w) c -> b c h w\", h=GS)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MaskTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_cls,\n",
    "        patch_size,\n",
    "        d_encoder,\n",
    "        n_layers,\n",
    "        n_heads,\n",
    "        d_model,\n",
    "        d_ff,\n",
    "        drop_path_rate,\n",
    "        dropout,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_encoder = d_encoder\n",
    "        self.patch_size = patch_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_cls = n_cls\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.scale = d_model ** -0.5\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, n_layers)]\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Block(d_model, n_heads, d_ff, dropout, dpr[i]) for i in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        self.cls_emb = nn.Parameter(torch.randn(1, n_cls, d_model))\n",
    "        self.proj_dec = nn.Linear(d_encoder, d_model)\n",
    "\n",
    "        self.proj_patch = nn.Parameter(self.scale * torch.randn(d_model, d_model))\n",
    "        self.proj_classes = nn.Parameter(self.scale * torch.randn(d_model, d_model))\n",
    "\n",
    "        self.decoder_norm = nn.LayerNorm(d_model)\n",
    "        self.mask_norm = nn.LayerNorm(n_cls)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "        trunc_normal_(self.cls_emb, std=0.02)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {\"cls_emb\"}\n",
    "\n",
    "    def forward(self, x, im_size):\n",
    "        H, W = im_size\n",
    "        GS = H // self.patch_size\n",
    "\n",
    "        x = self.proj_dec(x)\n",
    "        cls_emb = self.cls_emb.expand(x.size(0), -1, -1)\n",
    "        x = torch.cat((x, cls_emb), 1)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        patches, cls_seg_feat = x[:, : -self.n_cls], x[:, -self.n_cls :]\n",
    "        patches = patches @ self.proj_patch\n",
    "        cls_seg_feat = cls_seg_feat @ self.proj_classes\n",
    "\n",
    "        patches = patches / patches.norm(dim=-1, keepdim=True)\n",
    "        cls_seg_feat = cls_seg_feat / cls_seg_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        masks = patches @ cls_seg_feat.transpose(1, 2)\n",
    "        masks = self.mask_norm(masks)\n",
    "        masks = rearrange(masks, \"b (h w) n -> b n h w\", h=int(GS))\n",
    "\n",
    "        return masks\n",
    "\n",
    "    def get_attention_map(self, x, layer_id):\n",
    "        if layer_id >= self.n_layers or layer_id < 0:\n",
    "            raise ValueError(\n",
    "                f\"Provided layer_id: {layer_id} is not valid. 0 <= {layer_id} < {self.n_layers}.\"\n",
    "            )\n",
    "        x = self.proj_dec(x)\n",
    "        cls_emb = self.cls_emb.expand(x.size(0), -1, -1)\n",
    "        x = torch.cat((x, cls_emb), 1)\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            if i < layer_id:\n",
    "                x = blk(x)\n",
    "            else:\n",
    "                return blk(x, return_attention=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from 2020 Ross Wightman\n",
    "https://github.com/rwightman/pytorch-image-models\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.layers import DropPath\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.models.vision_transformer import _load_weights\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, embed_dim, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        if image_size[0] % patch_size != 0 or image_size[1] % patch_size != 0:\n",
    "            raise ValueError(\"image dimensions must be divisible by the patch size\")\n",
    "        self.grid_size = image_size[0] // patch_size, image_size[1] // patch_size\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            channels, embed_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, im):\n",
    "        B, C, H, W = im.shape\n",
    "        x = self.proj(im).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        patch_size,\n",
    "        n_layers,\n",
    "        d_model,\n",
    "        d_ff,\n",
    "        n_heads,\n",
    "        n_cls,\n",
    "        dropout=0.1,\n",
    "        drop_path_rate=0.0,\n",
    "        distilled=False,\n",
    "        channels=3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            image_size,\n",
    "            patch_size,\n",
    "            d_model,\n",
    "            channels,\n",
    "        )\n",
    "        self.patch_size = patch_size\n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.n_cls = n_cls\n",
    "\n",
    "        # cls and pos tokens\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.distilled = distilled\n",
    "        if self.distilled:\n",
    "            self.dist_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "            self.pos_embed = nn.Parameter(\n",
    "                torch.randn(1, self.patch_embed.num_patches + 2, d_model)\n",
    "            )\n",
    "            self.head_dist = nn.Linear(d_model, n_cls)\n",
    "        else:\n",
    "            self.pos_embed = nn.Parameter(\n",
    "                torch.randn(1, self.patch_embed.num_patches + 1, d_model)\n",
    "            )\n",
    "\n",
    "        # transformer blocks\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, n_layers)]\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Block(d_model, n_heads, d_ff, dropout, dpr[i]) for i in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        # output head\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, n_cls)\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=0.02)\n",
    "        trunc_normal_(self.cls_token, std=0.02)\n",
    "        if self.distilled:\n",
    "            trunc_normal_(self.dist_token, std=0.02)\n",
    "        self.pre_logits = nn.Identity()\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {\"pos_embed\", \"cls_token\", \"dist_token\"}\n",
    "\n",
    "    @torch.jit.ignore()\n",
    "    def load_pretrained(self, checkpoint_path, prefix=\"\"):\n",
    "        _load_weights(self, checkpoint_path, prefix)\n",
    "\n",
    "    def forward(self, im, return_features=False):\n",
    "        B, _, H, W = im.shape\n",
    "        PS = self.patch_size\n",
    "\n",
    "        x = self.patch_embed(im)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        if self.distilled:\n",
    "            dist_tokens = self.dist_token.expand(B, -1, -1)\n",
    "            x = torch.cat((cls_tokens, dist_tokens, x), dim=1)\n",
    "        else:\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        pos_embed = self.pos_embed\n",
    "        num_extra_tokens = 1 + self.distilled\n",
    "        if x.shape[1] != pos_embed.shape[1]:\n",
    "            pos_embed = resize_pos_embed(\n",
    "                pos_embed,\n",
    "                self.patch_embed.grid_size,\n",
    "                (H // PS, W // PS),\n",
    "                num_extra_tokens,\n",
    "            )\n",
    "        x = x + pos_embed\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if return_features:\n",
    "            return x\n",
    "\n",
    "        if self.distilled:\n",
    "            x, x_dist = x[:, 0], x[:, 1]\n",
    "            x = self.head(x)\n",
    "            x_dist = self.head_dist(x_dist)\n",
    "            x = (x + x_dist) / 2\n",
    "        else:\n",
    "            x = x[:, 0]\n",
    "            x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def get_attention_map(self, im, layer_id):\n",
    "        if layer_id >= self.n_layers or layer_id < 0:\n",
    "            raise ValueError(\n",
    "                f\"Provided layer_id: {layer_id} is not valid. 0 <= {layer_id} < {self.n_layers}.\"\n",
    "            )\n",
    "        B, _, H, W = im.shape\n",
    "        PS = self.patch_size\n",
    "\n",
    "        x = self.patch_embed(im)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        if self.distilled:\n",
    "            dist_tokens = self.dist_token.expand(B, -1, -1)\n",
    "            x = torch.cat((cls_tokens, dist_tokens, x), dim=1)\n",
    "        else:\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        pos_embed = self.pos_embed\n",
    "        num_extra_tokens = 1 + self.distilled\n",
    "        if x.shape[1] != pos_embed.shape[1]:\n",
    "            pos_embed = resize_pos_embed(\n",
    "                pos_embed,\n",
    "                self.patch_embed.grid_size,\n",
    "                (H // PS, W // PS),\n",
    "                num_extra_tokens,\n",
    "            )\n",
    "        x = x + pos_embed\n",
    "\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            if i < layer_id:\n",
    "                x = blk(x)\n",
    "            else:\n",
    "                return blk(x, return_attention=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmenter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import trunc_normal_\n",
    "\n",
    "class Segmenter(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        n_cls,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_cls = n_cls\n",
    "        self.patch_size = encoder.patch_size\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        def append_prefix_no_weight_decay(prefix, module):\n",
    "            return set(map(lambda x: prefix + x, module.no_weight_decay()))\n",
    "\n",
    "        nwd_params = append_prefix_no_weight_decay(\"encoder.\", self.encoder).union(\n",
    "            append_prefix_no_weight_decay(\"decoder.\", self.decoder)\n",
    "        )\n",
    "        return nwd_params\n",
    "\n",
    "    def forward(self, im):\n",
    "        H_ori, W_ori = im.size(2), im.size(3)\n",
    "        im = padding(im, self.patch_size)\n",
    "        H, W = im.size(2), im.size(3)\n",
    "\n",
    "        x = self.encoder(im, return_features=True)\n",
    "\n",
    "        # remove CLS/DIST tokens for decoding\n",
    "        num_extra_tokens = 1 + self.encoder.distilled\n",
    "        x = x[:, num_extra_tokens:]\n",
    "\n",
    "        masks = self.decoder(x, (H, W))\n",
    "\n",
    "        masks = F.interpolate(masks, size=(H, W), mode=\"bilinear\")\n",
    "        masks = unpadding(masks, (H_ori, W_ori))\n",
    "\n",
    "        return masks\n",
    "\n",
    "    def get_attention_map_enc(self, im, layer_id):\n",
    "        return self.encoder.get_attention_map(im, layer_id)\n",
    "\n",
    "    def get_attention_map_dec(self, im, layer_id):\n",
    "        x = self.encoder(im, return_features=True)\n",
    "\n",
    "        # remove CLS/DIST tokens for decoding\n",
    "        num_extra_tokens = 1 + self.encoder.distilled\n",
    "        x = x[:, num_extra_tokens:]\n",
    "\n",
    "        return self.decoder.get_attention_map(x, layer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "from timm.models.helpers import load_pretrained, load_custom_pretrained\n",
    "from timm.models.vision_transformer import default_cfgs\n",
    "from timm.models.registry import register_model\n",
    "from timm.models.vision_transformer import _create_vision_transformer\n",
    "\n",
    "\n",
    "@register_model\n",
    "def vit_base_patch8_384(pretrained=False, **kwargs):\n",
    "    \"\"\"ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.\n",
    "    \"\"\"\n",
    "    model_kwargs = dict(patch_size=8, embed_dim=768, depth=12, num_heads=12, **kwargs)\n",
    "    model = _create_vision_transformer(\n",
    "        \"vit_base_patch8_384\",\n",
    "        pretrained=pretrained,\n",
    "        default_cfg=dict(\n",
    "            url=\"\",\n",
    "            input_size=(3, 384, 384),\n",
    "            mean=(0.5, 0.5, 0.5),\n",
    "            std=(0.5, 0.5, 0.5),\n",
    "            num_classes=1000,\n",
    "        ),\n",
    "        **model_kwargs,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_vit(model_cfg):\n",
    "    model_cfg = model_cfg.copy()\n",
    "    backbone = model_cfg.pop(\"backbone\")\n",
    "\n",
    "    normalization = model_cfg.pop(\"normalization\")\n",
    "    model_cfg[\"n_cls\"] = 1000\n",
    "    mlp_expansion_ratio = 4\n",
    "    model_cfg[\"d_ff\"] = mlp_expansion_ratio * model_cfg[\"d_model\"]\n",
    "\n",
    "    if backbone in default_cfgs:\n",
    "        default_cfg = default_cfgs[backbone]\n",
    "    else:\n",
    "        default_cfg = dict(\n",
    "            pretrained=False,\n",
    "            num_classes=1000,\n",
    "            drop_rate=0.0,\n",
    "            drop_path_rate=0.0,\n",
    "            drop_block_rate=None,\n",
    "        )\n",
    "\n",
    "    default_cfg[\"input_size\"] = (\n",
    "        3,\n",
    "        model_cfg[\"image_size\"][0],\n",
    "        model_cfg[\"image_size\"][1],\n",
    "    )\n",
    "    model = VisionTransformer(**model_cfg)\n",
    "    if backbone == \"vit_base_patch8_384\":\n",
    "        path = os.path.expandvars(\"$TORCH_HOME/hub/checkpoints/vit_base_patch8_384.pth\")\n",
    "        state_dict = torch.load(path, map_location=\"cpu\")\n",
    "        filtered_dict = checkpoint_filter_fn(state_dict, model)\n",
    "        model.load_state_dict(filtered_dict, strict=True)\n",
    "    elif \"deit\" in backbone:\n",
    "        load_pretrained(model, default_cfg, filter_fn=checkpoint_filter_fn)\n",
    "    else:\n",
    "        load_custom_pretrained(model, default_cfg)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_decoder(encoder, decoder_cfg):\n",
    "    decoder_cfg = decoder_cfg.copy()\n",
    "    name = decoder_cfg.pop(\"name\")\n",
    "    decoder_cfg[\"d_encoder\"] = encoder.d_model\n",
    "    decoder_cfg[\"patch_size\"] = encoder.patch_size\n",
    "\n",
    "    if \"linear\" in name:\n",
    "        decoder = DecoderLinear(**decoder_cfg)\n",
    "    elif name == \"mask_transformer\":\n",
    "        dim = encoder.d_model\n",
    "        n_heads = dim // 64\n",
    "        decoder_cfg[\"n_heads\"] = n_heads\n",
    "        decoder_cfg[\"d_model\"] = dim\n",
    "        decoder_cfg[\"d_ff\"] = 4 * dim\n",
    "        decoder = MaskTransformer(**decoder_cfg)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown decoder: {name}\")\n",
    "    return decoder\n",
    "\n",
    "\n",
    "def create_segmenter(model_cfg):\n",
    "    model_cfg = model_cfg.copy()\n",
    "    decoder_cfg = model_cfg.pop(\"decoder\")\n",
    "    decoder_cfg[\"n_cls\"] = model_cfg[\"n_cls\"]\n",
    "\n",
    "    encoder = create_vit(model_cfg)\n",
    "    decoder = create_decoder(encoder, decoder_cfg)\n",
    "    model = Segmenter(encoder, decoder, n_cls=model_cfg[\"n_cls\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_model(model_path):\n",
    "    variant_path = Path(model_path).parent / \"variant.yml\"\n",
    "    with open(variant_path, \"r\") as f:\n",
    "        variant = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    net_kwargs = variant[\"net_kwargs\"]\n",
    "\n",
    "    model = create_segmenter(net_kwargs)\n",
    "    data = torch.load(model_path, map_location=device)\n",
    "    checkpoint = data[\"model\"]\n",
    "\n",
    "    model.load_state_dict(checkpoint, strict=True)\n",
    "\n",
    "    return model, variant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
