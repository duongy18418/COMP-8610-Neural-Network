{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f747d23",
   "metadata": {
    "papermill": {
     "duration": 0.007598,
     "end_time": "2024-07-18T00:24:09.855251",
     "exception": false,
     "start_time": "2024-07-18T00:24:09.847653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Segmenter\n",
    "Author Paper: https://arxiv.org/abs/2105.05633"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a360b738",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T00:24:09.871691Z",
     "iopub.status.busy": "2024-07-18T00:24:09.871324Z",
     "iopub.status.idle": "2024-07-18T00:24:27.212660Z",
     "shell.execute_reply": "2024-07-18T00:24:27.211121Z"
    },
    "papermill": {
     "duration": 17.35229,
     "end_time": "2024-07-18T00:24:27.215292",
     "exception": false,
     "start_time": "2024-07-18T00:24:09.863002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\r\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (1.0.7)\r\n",
      "Collecting imutils\r\n",
      "  Downloading imutils-0.5.4.tar.gz (17 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2+cpu)\r\n",
      "Collecting lightning\r\n",
      "  Downloading lightning-2.3.3-py3-none-any.whl.metadata (35 kB)\r\n",
      "Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.4.0.post0)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from timm) (2.1.2+cpu)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm) (6.0.1)\r\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from timm) (0.23.4)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm) (0.4.3)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.32.3)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->timm) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->timm) (1.13.0)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->timm) (2024.5.0)\r\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.11.3.post0)\r\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\r\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.66.4)\r\n",
      "Requirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.3.3)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.9.1)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (69.0.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning) (3.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.7.4)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->timm) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\r\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading lightning-2.3.3-py3-none-any.whl (808 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m808.5/808.5 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: imutils\r\n",
      "  Building wheel for imutils (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25834 sha256=cab270a4b6975933f46a57a063d10c1c8a2e9aaa56d1e552d506578f6da7e23b\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/85/cf/3a/e265e975a1e7c7e54eb3692d6aa4e2e7d6a3945d29da46f2d7\r\n",
      "Successfully built imutils\r\n",
      "Installing collected packages: imutils, einops, lightning\r\n",
      "Successfully installed einops-0.8.0 imutils-0.5.4 lightning-2.3.3\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install einops timm imutils torchvision lightning torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86262eac",
   "metadata": {
    "papermill": {
     "duration": 0.008244,
     "end_time": "2024-07-18T00:24:27.232304",
     "exception": false,
     "start_time": "2024-07-18T00:24:27.224060",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Segmenter implemtation\n",
    "Implementation is adopted from https://github.com/rstrudel/segmenter/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f956dd1f",
   "metadata": {
    "papermill": {
     "duration": 0.008307,
     "end_time": "2024-07-18T00:24:27.249084",
     "exception": false,
     "start_time": "2024-07-18T00:24:27.240777",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495f6e71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T00:24:27.268400Z",
     "iopub.status.busy": "2024-07-18T00:24:27.267643Z",
     "iopub.status.idle": "2024-07-18T00:24:33.695662Z",
     "shell.execute_reply": "2024-07-18T00:24:33.694517Z"
    },
    "papermill": {
     "duration": 6.440785,
     "end_time": "2024-07-18T00:24:33.698340",
     "exception": false,
     "start_time": "2024-07-18T00:24:27.257555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from 2020 Ross Wightman\n",
    "https://github.com/rwightman/pytorch-image-models\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import DropPath\n",
    "import torch\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout, out_dim=None):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        if out_dim is None:\n",
    "            out_dim = dim\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    @property\n",
    "    def unwrapped(self):\n",
    "        return self\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        head_dim = dim // heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.attn = None\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "\n",
    "    @property\n",
    "    def unwrapped(self):\n",
    "        return self\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, N, C = x.shape\n",
    "        qkv = (\n",
    "            self.qkv(x)\n",
    "            .reshape(B, N, 3, self.heads, C // self.heads)\n",
    "            .permute(2, 0, 3, 1, 4)\n",
    "        )\n",
    "        \n",
    "        q, k, v = (\n",
    "            qkv[0],\n",
    "            qkv[1],\n",
    "            qkv[2],\n",
    "        )\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "       \n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "    \n",
    "        return x, attn\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_dim, dropout, drop_path):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, heads, dropout)\n",
    "        self.mlp = FeedForward(dim, mlp_dim, dropout)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        y, attn = self.attn(x, mask)\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcb77037",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T00:24:33.718439Z",
     "iopub.status.busy": "2024-07-18T00:24:33.717598Z",
     "iopub.status.idle": "2024-07-18T00:24:33.750317Z",
     "shell.execute_reply": "2024-07-18T00:24:33.749065Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.045484,
     "end_time": "2024-07-18T00:24:33.752652",
     "exception": false,
     "start_time": "2024-07-18T00:24:33.707168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from timm.models.layers import trunc_normal_\n",
    "\n",
    "##### outer folder utility\n",
    "import os\n",
    "import torch\n",
    "\"\"\"\n",
    "GPU wrappers\n",
    "\"\"\"\n",
    "\n",
    "use_gpu = False\n",
    "gpu_id = 0\n",
    "device = None\n",
    "\n",
    "distributed = False\n",
    "dist_rank = 0\n",
    "world_size = 1\n",
    "\n",
    "\n",
    "def set_gpu_mode(mode):\n",
    "    global use_gpu\n",
    "    global device\n",
    "    global gpu_id\n",
    "    global distributed\n",
    "    global dist_rank\n",
    "    global world_size\n",
    "    gpu_id = int(os.environ.get(\"SLURM_LOCALID\", 0))\n",
    "    dist_rank = int(os.environ.get(\"SLURM_PROCID\", 0))\n",
    "    world_size = int(os.environ.get(\"SLURM_NTASKS\", 1))\n",
    "\n",
    "    distributed = world_size > 1\n",
    "    use_gpu = mode\n",
    "    device = torch.device(f\"cuda:{gpu_id}\" if use_gpu else \"cpu\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        trunc_normal_(m.weight, std=0.02)\n",
    "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "        nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "\n",
    "def resize_pos_embed(posemb, grid_old_shape, grid_new_shape, num_extra_tokens):\n",
    "    # Rescale the grid of position embeddings when loading from state_dict. Adapted from\n",
    "    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224\n",
    "    posemb_tok, posemb_grid = (\n",
    "        posemb[:, :num_extra_tokens],\n",
    "        posemb[0, num_extra_tokens:],\n",
    "    )\n",
    "    if grid_old_shape is None:\n",
    "        gs_old_h = int(math.sqrt(len(posemb_grid)))\n",
    "        gs_old_w = gs_old_h\n",
    "    else:\n",
    "        gs_old_h, gs_old_w = grid_old_shape\n",
    "\n",
    "    gs_h, gs_w = grid_new_shape\n",
    "    posemb_grid = posemb_grid.reshape(1, gs_old_h, gs_old_w, -1).permute(0, 3, 1, 2)\n",
    "    posemb_grid = F.interpolate(posemb_grid, size=(gs_h, gs_w), mode=\"bilinear\")\n",
    "    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, gs_h * gs_w, -1)\n",
    "    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)\n",
    "    return posemb\n",
    "\n",
    "\n",
    "def checkpoint_filter_fn(state_dict, model):\n",
    "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
    "    out_dict = {}\n",
    "    if \"model\" in state_dict:\n",
    "        # For deit models\n",
    "        state_dict = state_dict[\"model\"]\n",
    "    num_extra_tokens = 1 + (\"dist_token\" in state_dict.keys())\n",
    "    patch_size = model.patch_size\n",
    "    image_size = model.patch_embed.image_size\n",
    "    for k, v in state_dict.items():\n",
    "        if k == \"pos_embed\" and v.shape != model.pos_embed.shape:\n",
    "            # To resize pos embedding when using model at different size from pretrained weights\n",
    "            v = resize_pos_embed(\n",
    "                v,\n",
    "                None,\n",
    "                (image_size[0] // patch_size, image_size[1] // patch_size),\n",
    "                num_extra_tokens,\n",
    "            )\n",
    "        out_dict[k] = v\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "def padding(im, patch_size, fill_value=0):\n",
    "    # make the image sizes divisible by patch_size\n",
    "    H, W = im.size(2), im.size(3)\n",
    "    pad_h, pad_w = 0, 0\n",
    "    if H % patch_size > 0:\n",
    "        pad_h = patch_size - (H % patch_size)\n",
    "    if W % patch_size > 0:\n",
    "        pad_w = patch_size - (W % patch_size)\n",
    "    im_padded = im\n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        im_padded = F.pad(im, (0, pad_w, 0, pad_h), value=fill_value)\n",
    "    return im_padded\n",
    "\n",
    "\n",
    "def unpadding(y, target_size):\n",
    "    H, W = target_size\n",
    "    H_pad, W_pad = y.size(2), y.size(3)\n",
    "    # crop predictions on extra pixels coming from padding\n",
    "    extra_h = H_pad - H\n",
    "    extra_w = W_pad - W\n",
    "    if extra_h > 0:\n",
    "        y = y[:, :, :-extra_h]\n",
    "    if extra_w > 0:\n",
    "        y = y[:, :, :, :-extra_w]\n",
    "    return y\n",
    "\n",
    "\n",
    "def resize(im, smaller_size):\n",
    "    h, w = im.shape[2:]\n",
    "    if h < w:\n",
    "        ratio = w / h\n",
    "        h_res, w_res = smaller_size, ratio * smaller_size\n",
    "    else:\n",
    "        ratio = h / w\n",
    "        h_res, w_res = ratio * smaller_size, smaller_size\n",
    "    if min(h, w) < smaller_size:\n",
    "        im_res = F.interpolate(im, (int(h_res), int(w_res)), mode=\"bilinear\")\n",
    "    else:\n",
    "        im_res = im\n",
    "    return im_res\n",
    "\n",
    "\n",
    "def sliding_window(im, flip, window_size, window_stride):\n",
    "    B, C, H, W = im.shape\n",
    "    ws = window_size\n",
    "\n",
    "    windows = {\"crop\": [], \"anchors\": []}\n",
    "    h_anchors = torch.arange(0, H, window_stride)\n",
    "    w_anchors = torch.arange(0, W, window_stride)\n",
    "    h_anchors = [h.item() for h in h_anchors if h < H - ws] + [H - ws]\n",
    "    w_anchors = [w.item() for w in w_anchors if w < W - ws] + [W - ws]\n",
    "    for ha in h_anchors:\n",
    "        for wa in w_anchors:\n",
    "            window = im[:, :, ha : ha + ws, wa : wa + ws]\n",
    "            windows[\"crop\"].append(window)\n",
    "            windows[\"anchors\"].append((ha, wa))\n",
    "    windows[\"flip\"] = flip\n",
    "    windows[\"shape\"] = (H, W)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def merge_windows(windows, window_size, ori_shape):\n",
    "    ws = window_size\n",
    "    im_windows = windows[\"seg_maps\"]\n",
    "    anchors = windows[\"anchors\"]\n",
    "    C = im_windows[0].shape[0]\n",
    "    H, W = windows[\"shape\"]\n",
    "    flip = windows[\"flip\"]\n",
    "\n",
    "    logit = torch.zeros((C, H, W), device=im_windows.device)\n",
    "    count = torch.zeros((1, H, W), device=im_windows.device)\n",
    "    for window, (ha, wa) in zip(im_windows, anchors):\n",
    "        logit[:, ha : ha + ws, wa : wa + ws] += window\n",
    "        count[:, ha : ha + ws, wa : wa + ws] += 1\n",
    "    logit = logit / count\n",
    "    logit = F.interpolate(\n",
    "        logit.unsqueeze(0),\n",
    "        ori_shape,\n",
    "        mode=\"bilinear\",\n",
    "    )[0]\n",
    "    if flip:\n",
    "        logit = torch.flip(logit, (2,))\n",
    "    result = F.softmax(logit, 0)\n",
    "    return result\n",
    "\n",
    "\n",
    "def inference(\n",
    "    model,\n",
    "    ims,\n",
    "    ims_metas,\n",
    "    ori_shape,\n",
    "    window_size,\n",
    "    window_stride,\n",
    "    batch_size,\n",
    "):\n",
    "    C = model.n_cls\n",
    "    seg_map = torch.zeros((C, ori_shape[0], ori_shape[1]), device=device)\n",
    "    for im, im_metas in zip(ims, ims_metas):\n",
    "        im = im.to(device)\n",
    "        im = resize(im, window_size)\n",
    "        flip = im_metas[\"flip\"]\n",
    "        windows = sliding_window(im, flip, window_size, window_stride)\n",
    "        crops = torch.stack(windows.pop(\"crop\"))[:, 0]\n",
    "        B = len(crops)\n",
    "        WB = batch_size\n",
    "        seg_maps = torch.zeros((B, C, window_size, window_size), device=im.device)\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, B, WB):\n",
    "                seg_maps[i : i + WB] = model.forward(crops[i : i + WB])\n",
    "        windows[\"seg_maps\"] = seg_maps\n",
    "        im_seg_map = merge_windows(windows, window_size, ori_shape)\n",
    "        seg_map += im_seg_map\n",
    "    seg_map /= len(ims)\n",
    "    return seg_map\n",
    "\n",
    "\n",
    "def num_params(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    n_params = sum([torch.prod(torch.tensor(p.size())) for p in model_parameters])\n",
    "    return n_params.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b04c3b9",
   "metadata": {
    "papermill": {
     "duration": 0.008114,
     "end_time": "2024-07-18T00:24:33.769421",
     "exception": false,
     "start_time": "2024-07-18T00:24:33.761307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a9b6b53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T00:24:33.788153Z",
     "iopub.status.busy": "2024-07-18T00:24:33.787752Z",
     "iopub.status.idle": "2024-07-18T00:24:33.806227Z",
     "shell.execute_reply": "2024-07-18T00:24:33.805116Z"
    },
    "papermill": {
     "duration": 0.030746,
     "end_time": "2024-07-18T00:24:33.808677",
     "exception": false,
     "start_time": "2024-07-18T00:24:33.777931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from timm.models.layers import trunc_normal_\n",
    "\n",
    "class DecoderLinear(nn.Module):\n",
    "    def __init__(self, n_cls, patch_size, d_encoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_encoder = d_encoder\n",
    "        self.patch_size = patch_size\n",
    "        self.n_cls = n_cls\n",
    "\n",
    "        self.head = nn.Linear(self.d_encoder, n_cls)\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return set()\n",
    "\n",
    "    def forward(self, x, im_size):\n",
    "        H, W = im_size\n",
    "        GS = H // self.patch_size\n",
    "        x = self.head(x)\n",
    "        ## h*w = n\n",
    "        x = rearrange(x, \"b (h w) c -> b c h w\", h=GS)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MaskTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_cls,\n",
    "        patch_size,\n",
    "        d_encoder,\n",
    "        n_layers,\n",
    "        n_heads,\n",
    "        d_model,\n",
    "        d_ff,\n",
    "        drop_path_rate,\n",
    "        dropout,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_encoder = d_encoder # return by decode n x d\n",
    "        self.patch_size = patch_size \n",
    "        self.n_layers = n_layers\n",
    "        self.n_cls = n_cls\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.scale = d_model ** -0.5 # attention\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, n_layers)]\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Block(d_model, n_heads, d_ff, dropout, dpr[i]) for i in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        # initial class embedding randomly. Shape K x D_model. Parameter means learnable\n",
    "        self.cls_emb = nn.Parameter(torch.randn(1, n_cls, d_model))\n",
    "        # Projection: map D of encoder to D model\n",
    "        self.proj_dec = nn.Linear(d_encoder, d_model)\n",
    "\n",
    "        self.proj_patch = nn.Parameter(self.scale * torch.randn(d_model, d_model))\n",
    "        self.proj_classes = nn.Parameter(self.scale * torch.randn(d_model, d_model))\n",
    "\n",
    "        self.decoder_norm = nn.LayerNorm(d_model)\n",
    "        self.mask_norm = nn.LayerNorm(n_cls)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "        trunc_normal_(self.cls_emb, std=0.02)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {\"cls_emb\"}\n",
    "    \n",
    "    def forward(self, x, im_size):\n",
    "        H, W = im_size\n",
    "        GS = H // self.patch_size\n",
    "\n",
    "        x = self.proj_dec(x)\n",
    "        cls_emb = self.cls_emb.expand(x.size(0), -1, -1)\n",
    "        x = torch.cat((x, cls_emb), 1)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        patches, cls_seg_feat = x[:, : -self.n_cls], x[:, -self.n_cls :]\n",
    "        patches = patches @ self.proj_patch\n",
    "        cls_seg_feat = cls_seg_feat @ self.proj_classes\n",
    "\n",
    "        patches = patches / patches.norm(dim=-1, keepdim=True)\n",
    "        cls_seg_feat = cls_seg_feat / cls_seg_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        masks = patches @ cls_seg_feat.transpose(1, 2)\n",
    "        masks = self.mask_norm(masks)\n",
    "        masks = rearrange(masks, \"b (h w) n -> b n h w\", h=int(GS))\n",
    "\n",
    "        return masks\n",
    "\n",
    "    def get_attention_map(self, x, layer_id):\n",
    "        if layer_id >= self.n_layers or layer_id < 0:\n",
    "            raise ValueError(\n",
    "                f\"Provided layer_id: {layer_id} is not valid. 0 <= {layer_id} < {self.n_layers}.\"\n",
    "            )\n",
    "        x = self.proj_dec(x)\n",
    "        cls_emb = self.cls_emb.expand(x.size(0), -1, -1)\n",
    "        x = torch.cat((x, cls_emb), 1)\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            if i < layer_id:\n",
    "                x = blk(x)\n",
    "            else:\n",
    "                return blk(x, return_attention=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a356b9",
   "metadata": {
    "papermill": {
     "duration": 0.008052,
     "end_time": "2024-07-18T00:24:33.825249",
     "exception": false,
     "start_time": "2024-07-18T00:24:33.817197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Vision transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52368d22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T00:24:33.845036Z",
     "iopub.status.busy": "2024-07-18T00:24:33.844654Z",
     "iopub.status.idle": "2024-07-18T00:24:34.103914Z",
     "shell.execute_reply": "2024-07-18T00:24:34.102886Z"
    },
    "papermill": {
     "duration": 0.272203,
     "end_time": "2024-07-18T00:24:34.106622",
     "exception": false,
     "start_time": "2024-07-18T00:24:33.834419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after patch emd: torch.Size([4, 64, 128])\n",
      "pos emb shape: torch.Size([1, 65, 128])\n",
      "cls tokens shape: torch.Size([4, 1, 128])\n",
      "after add cls tokens for to the patches: torch.Size([4, 65, 128])\n",
      "pos_embed after resize:  torch.Size([1, 65, 128])\n",
      "==== Multihead Attention Block\n",
      "- Attention Block\n",
      "mapping from B, N, d_model to BxNx3dim  torch.Size([4, 65, 384])\n",
      "reshape from  B,N,3xdim to B,N,3,heads,d_model / heads  torch.Size([4, 65, 3, 8, 16])\n",
      "permute to 3,B,heads,n,d_model/heads torch.Size([3, 4, 8, 65, 16])\n",
      "q,k,v shape  torch.Size([4, 8, 65, 16])\n",
      "after attention x  torch.Size([4, 65, 128])\n",
      "- After Feedforward, Dropout of MA block x:  torch.Size([4, 65, 128])\n",
      "====Back to Fully connected head====\n",
      "Mask decoder input:  torch.Size([4, 65, 128])\n",
      "Finish vision transformer, output n-classes  torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Adapted from 2020 Ross Wightman\n",
    "https://github.com/rwightman/pytorch-image-models\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.models.vision_transformer import _load_weights\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, embed_dim, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        if image_size[0] % patch_size != 0 or image_size[1] % patch_size != 0:\n",
    "            raise ValueError(\"image dimensions must be divisible by the patch size\")\n",
    "        self.grid_size = image_size[0] // patch_size, image_size[1] // patch_size\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            channels, embed_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, im):\n",
    "        B, C, H, W = im.shape\n",
    "        x = self.proj(im).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        patch_size,\n",
    "        n_layers,\n",
    "        d_model,\n",
    "        d_ff,\n",
    "        n_heads,\n",
    "        n_cls,\n",
    "        dropout=0.1,\n",
    "        drop_path_rate=0.0,\n",
    "        distilled=False,\n",
    "        channels=3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            image_size,\n",
    "            patch_size,\n",
    "            d_model,\n",
    "            channels,\n",
    "        )\n",
    "        self.patch_size = patch_size\n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.n_cls = n_cls\n",
    "\n",
    "        # cls and pos tokens\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.distilled = distilled\n",
    "        if self.distilled:\n",
    "            self.dist_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "            self.pos_embed = nn.Parameter(\n",
    "                torch.randn(1, self.patch_embed.num_patches + 2, d_model)\n",
    "            )\n",
    "            self.head_dist = nn.Linear(d_model, n_cls)\n",
    "        else:\n",
    "            self.pos_embed = nn.Parameter(\n",
    "                torch.randn(1, self.patch_embed.num_patches + 1, d_model)\n",
    "            )\n",
    "\n",
    "        # transformer blocks\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, n_layers)]\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Block(d_model, n_heads, d_ff, dropout, dpr[i]) for i in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        # output head\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, n_cls)\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=0.02)\n",
    "        trunc_normal_(self.cls_token, std=0.02)\n",
    "        if self.distilled:\n",
    "            trunc_normal_(self.dist_token, std=0.02)\n",
    "        self.pre_logits = nn.Identity()\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {\"pos_embed\", \"cls_token\", \"dist_token\"}\n",
    "\n",
    "    @torch.jit.ignore()\n",
    "    def load_pretrained(self, checkpoint_path, prefix=\"\"):\n",
    "        _load_weights(self, checkpoint_path, prefix)\n",
    "\n",
    "    def forward(self, im, return_features=False):\n",
    "        B, _, H, W = im.shape\n",
    "        PS = self.patch_size\n",
    "\n",
    "        x = self.patch_embed(im)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        if self.distilled:\n",
    "            dist_tokens = self.dist_token.expand(B, -1, -1)\n",
    "            x = torch.cat((cls_tokens, dist_tokens, x), dim=1)\n",
    "        else:\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        pos_embed = self.pos_embed\n",
    "        num_extra_tokens = 1 + self.distilled\n",
    "        if x.shape[1] != pos_embed.shape[1]:\n",
    "            pos_embed = resize_pos_embed(\n",
    "                pos_embed,\n",
    "                self.patch_embed.grid_size,\n",
    "                (H // PS, W // PS),\n",
    "                num_extra_tokens,\n",
    "            )\n",
    "        x = x + pos_embed\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if return_features:\n",
    "            return x\n",
    "\n",
    "        if self.distilled:\n",
    "            x, x_dist = x[:, 0], x[:, 1]\n",
    "            x = self.head(x)\n",
    "            x_dist = self.head_dist(x_dist)\n",
    "            x = (x + x_dist) / 2\n",
    "        else:\n",
    "            x = x[:, 0]\n",
    "            x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def get_attention_map(self, im, layer_id):\n",
    "        if layer_id >= self.n_layers or layer_id < 0:\n",
    "            raise ValueError(\n",
    "                f\"Provided layer_id: {layer_id} is not valid. 0 <= {layer_id} < {self.n_layers}.\"\n",
    "            )\n",
    "        B, _, H, W = im.shape\n",
    "        PS = self.patch_size\n",
    "\n",
    "        x = self.patch_embed(im)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        if self.distilled:\n",
    "            dist_tokens = self.dist_token.expand(B, -1, -1)\n",
    "            x = torch.cat((cls_tokens, dist_tokens, x), dim=1)\n",
    "        else:\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        pos_embed = self.pos_embed\n",
    "        num_extra_tokens = 1 + self.distilled\n",
    "        if x.shape[1] != pos_embed.shape[1]:\n",
    "            pos_embed = resize_pos_embed(\n",
    "                pos_embed,\n",
    "                self.patch_embed.grid_size,\n",
    "                (H // PS, W // PS),\n",
    "                num_extra_tokens,\n",
    "            )\n",
    "        x = x + pos_embed\n",
    "\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            if i < layer_id:\n",
    "                x = blk(x)\n",
    "            else:\n",
    "                return blk(x, return_attention=True)\n",
    "\n",
    "\n",
    "d_model = 128\n",
    "batch = 4\n",
    "channels = 3\n",
    "img_size = (320, 320)\n",
    "patch_size = 40\n",
    "drop_path_rate = .5\n",
    "n_layers = 1\n",
    "n_heads = 8\n",
    "d_ff = 128\n",
    "\n",
    "n_cls = 10\n",
    "pe = PatchEmbedding(img_size, patch_size, d_model, channels)\n",
    "distilled = False\n",
    "x = torch.rand(batch, channels, img_size[0], img_size[1])\n",
    "x = pe(x)\n",
    "print(\"after patch emd:\", x.shape)\n",
    "\n",
    "pos_embed = nn.Parameter(torch.randn(1, pe.num_patches + 1, d_model))\n",
    "print(\"pos emb shape:\", pos_embed.size())\n",
    "\n",
    "cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "cls_tokens = cls_token.expand(batch, -1, -1)\n",
    "print(\"cls tokens shape:\", cls_tokens.shape)\n",
    "\n",
    "cls_token.expand(batch, -1, -1)\n",
    "x = torch.cat((cls_tokens, x), dim=1) # This is why pos_emd has the second dimension is num_patches + 1.. We added cls_tokens \n",
    "\n",
    "print(\"after add cls tokens for to the patches:\", x.shape)\n",
    "\n",
    "dropout = nn.Dropout(0.5)\n",
    "\n",
    "num_extra_tokens = 1 + distilled\n",
    "if x.shape[1] != pos_embed.shape[1]:\n",
    "    pos_embed = resize_pos_embed(\n",
    "        pos_embed,\n",
    "        pe.grid_size,\n",
    "        (img_size[0] // patch_size, img_size[1] // patch_size),\n",
    "        num_extra_tokens,\n",
    "    )\n",
    "print('pos_embed after resize: ', pos_embed.shape)\n",
    "\n",
    "x = x + pos_embed\n",
    "x = dropout(x)\n",
    "\n",
    "dropout = 0.5\n",
    "dpr = [x.item() for x in torch.linspace(0, drop_path_rate, n_layers)]\n",
    "\n",
    "print(\"==== Multihead Attention Block\")\n",
    "dim = d_model\n",
    "\n",
    "norm1 = nn.LayerNorm(dim)\n",
    "x = norm1(x)\n",
    "\n",
    "y = torch.Tensor(x)\n",
    "heads = n_heads\n",
    "head_dim = dim // heads\n",
    "scale = head_dim ** -0.5\n",
    "attn = None\n",
    "\n",
    "## Attention\n",
    "print(\"- Attention Block\")\n",
    "qkv = nn.Linear(dim, dim * 3)\n",
    "attn_drop = nn.Dropout(dropout)\n",
    "proj = nn.Linear(dim, dim)\n",
    "proj_drop = nn.Dropout(dropout)\n",
    "\n",
    "B, N, C = x.shape\n",
    "\n",
    "projected = qkv(y)\n",
    "print(\"mapping from B, N, d_model to BxNx3dim \", projected.shape)\n",
    "projected = projected.reshape(B, N, 3, heads, C // heads)\n",
    "\n",
    "# n_heads used hear, to divide d_model to 2 dimensions: heads and d_model / heads => Multihead attention ?\n",
    "print(\"reshape from  B,N,3xdim to B,N,3,heads,d_model / heads \", projected.shape)\n",
    "projected = projected.permute(2, 0, 3, 1, 4)\n",
    "print(\"permute to 3,B,heads,n,d_model/heads\" , projected.shape)\n",
    "q, k, v = projected[0], projected[1], projected[2]\n",
    "print(\"q,k,v shape \", q.shape)\n",
    "\n",
    "attn = (q @ k.transpose(-2, -1)) * scale\n",
    "attn = attn.softmax(dim=-1)\n",
    "attn = attn_drop(attn)\n",
    "\n",
    "y = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "y = proj(y)\n",
    "y = proj_drop(y)\n",
    "print(\"after attention x \", y.shape)\n",
    "\n",
    "# back to block\n",
    "norm2 = nn.LayerNorm(dim)\n",
    "mlp = FeedForward(dim, d_ff, dropout)\n",
    "drop_path = DropPath(0.1) if 0.1 > 0.0 else nn.Identity()\n",
    "\n",
    "x = x + drop_path(y)\n",
    "x = x + drop_path(mlp(norm2(x)))\n",
    "\n",
    "print('- After Feedforward, Dropout of MA block x: ', x.shape)\n",
    "\n",
    "norm = nn.LayerNorm(d_model)\n",
    "head = nn.Linear(d_model, n_cls)\n",
    "\n",
    "print('====Back to Fully connected head====')\n",
    "x = norm(x)\n",
    "print(\"Mask decoder input: \", x.shape)\n",
    "x = x[:, 0]\n",
    "x = head(x)\n",
    "print(\"Finish vision transformer, output n-classes \", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c565a7",
   "metadata": {
    "papermill": {
     "duration": 0.00825,
     "end_time": "2024-07-18T00:24:34.123613",
     "exception": false,
     "start_time": "2024-07-18T00:24:34.115363",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Segmenter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a03bee38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T00:24:34.142858Z",
     "iopub.status.busy": "2024-07-18T00:24:34.142476Z",
     "iopub.status.idle": "2024-07-18T00:24:34.153569Z",
     "shell.execute_reply": "2024-07-18T00:24:34.152423Z"
    },
    "papermill": {
     "duration": 0.023707,
     "end_time": "2024-07-18T00:24:34.156066",
     "exception": false,
     "start_time": "2024-07-18T00:24:34.132359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import trunc_normal_\n",
    "\n",
    "class Segmenter(nn.Module):\n",
    "    def __init__(self, encoder, decoder, n_cls):\n",
    "        super().__init__()\n",
    "        self.n_cls = n_cls\n",
    "        self.patch_size = encoder.patch_size\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        def append_prefix_no_weight_decay(prefix, module):\n",
    "            return set(map(lambda x: prefix + x, module.no_weight_decay()))\n",
    "\n",
    "        nwd_params = append_prefix_no_weight_decay(\"encoder.\", self.encoder).union(\n",
    "            append_prefix_no_weight_decay(\"decoder.\", self.decoder)\n",
    "        )\n",
    "        return nwd_params\n",
    "\n",
    "    def forward(self, im):\n",
    "        H_ori, W_ori = im.size(2), im.size(3)\n",
    "        im = padding(im, self.patch_size)\n",
    "        H, W = im.size(2), im.size(3)\n",
    "\n",
    "        x = self.encoder(im, return_features=True)\n",
    "\n",
    "        # remove CLS/DIST tokens for decoding\n",
    "        num_extra_tokens = 1 + self.encoder.distilled\n",
    "        x = x[:, num_extra_tokens:]\n",
    "\n",
    "        masks = self.decoder(x, (H, W))\n",
    "\n",
    "        masks = F.interpolate(masks, size=(H, W), mode=\"bilinear\")\n",
    "        masks = unpadding(masks, (H_ori, W_ori))\n",
    "\n",
    "        return masks\n",
    "\n",
    "    def get_attention_map_enc(self, im, layer_id):\n",
    "        return self.encoder.get_attention_map(im, layer_id)\n",
    "\n",
    "    def get_attention_map_dec(self, im, layer_id):\n",
    "        x = self.encoder(im, return_features=True)\n",
    "\n",
    "        # remove CLS/DIST tokens for decoding\n",
    "        num_extra_tokens = 1 + self.encoder.distilled\n",
    "        x = x[:, num_extra_tokens:]\n",
    "\n",
    "        return self.decoder.get_attention_map(x, layer_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85044021",
   "metadata": {
    "papermill": {
     "duration": 0.008965,
     "end_time": "2024-07-18T00:24:34.173711",
     "exception": false,
     "start_time": "2024-07-18T00:24:34.164746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c421cd58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T00:24:34.192932Z",
     "iopub.status.busy": "2024-07-18T00:24:34.192545Z",
     "iopub.status.idle": "2024-07-18T00:24:34.212006Z",
     "shell.execute_reply": "2024-07-18T00:24:34.211077Z"
    },
    "papermill": {
     "duration": 0.032094,
     "end_time": "2024-07-18T00:24:34.214622",
     "exception": false,
     "start_time": "2024-07-18T00:24:34.182528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "import torch\n",
    "import os\n",
    "from timm.models.helpers import load_pretrained, load_custom_pretrained\n",
    "from timm.models.vision_transformer import default_cfgs\n",
    "from timm.models.registry import register_model\n",
    "from timm.models.vision_transformer import _create_vision_transformer\n",
    "\n",
    "\n",
    "@register_model\n",
    "def vit_base_patch8_384(pretrained=False, **kwargs):\n",
    "    \"\"\"ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.\n",
    "    \"\"\"\n",
    "    model_kwargs = dict(patch_size=8, embed_dim=768, depth=12, num_heads=12, **kwargs)\n",
    "    model = _create_vision_transformer(\n",
    "        \"vit_base_patch8_384\",\n",
    "        pretrained=pretrained,\n",
    "        default_cfg=dict(\n",
    "            url=\"\",\n",
    "            input_size=(3, 384, 384),\n",
    "            mean=(0.5, 0.5, 0.5),\n",
    "            std=(0.5, 0.5, 0.5),\n",
    "            num_classes=1000,\n",
    "        ),\n",
    "        **model_kwargs,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def create_vit(model_cfg):\n",
    "    model_cfg = model_cfg.copy()\n",
    "    backbone = model_cfg.pop(\"backbone\")\n",
    "\n",
    "    normalization = model_cfg.pop(\"normalization\")\n",
    "    model_cfg[\"n_cls\"] = 1000\n",
    "    mlp_expansion_ratio = 4\n",
    "    model_cfg[\"d_ff\"] = mlp_expansion_ratio * model_cfg[\"d_model\"]\n",
    "\n",
    "    if backbone in default_cfgs:\n",
    "        default_cfg = default_cfgs[backbone]\n",
    "    else:\n",
    "        default_cfg = dict(\n",
    "            pretrained=False,\n",
    "            num_classes=1000,\n",
    "            drop_rate=0.0,\n",
    "            drop_path_rate=0.0,\n",
    "            drop_block_rate=None,\n",
    "        )\n",
    "    \n",
    "    default_cfg[\"input_size\"] = (\n",
    "        3,\n",
    "        model_cfg[\"image_size\"][0],\n",
    "        model_cfg[\"image_size\"][1],\n",
    "    )\n",
    "\n",
    "    model = VisionTransformer(**model_cfg)\n",
    "    if backbone == \"vit_base_patch8_384\":\n",
    "        path = os.path.expandvars(\"$TORCH_HOME/hub/checkpoints/vit_base_patch8_384.pth\")\n",
    "        state_dict = torch.load(path, map_location=\"cpu\")\n",
    "        filtered_dict = checkpoint_filter_fn(state_dict, model)\n",
    "        model.load_state_dict(filtered_dict, strict=True)\n",
    "    elif \"deit\" in backbone:\n",
    "        load_pretrained(model, default_cfg, filter_fn=checkpoint_filter_fn)\n",
    "    else:\n",
    "        load_custom_pretrained(model, default_cfg)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def create_decoder(encoder, decoder_cfg):\n",
    "    decoder_cfg = decoder_cfg.copy()\n",
    "    name = decoder_cfg.pop(\"name\")\n",
    "    decoder_cfg[\"d_encoder\"] = encoder.d_model\n",
    "    decoder_cfg[\"patch_size\"] = encoder.patch_size\n",
    "\n",
    "    if \"linear\" in name:\n",
    "        decoder = DecoderLinear(**decoder_cfg)\n",
    "    elif name == \"mask_transformer\":\n",
    "        dim = encoder.d_model\n",
    "        n_heads = dim // 64\n",
    "        decoder_cfg[\"n_heads\"] = n_heads\n",
    "        decoder_cfg[\"d_model\"] = dim\n",
    "        decoder_cfg[\"d_ff\"] = 4 * dim\n",
    "        decoder = MaskTransformer(**decoder_cfg)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown decoder: {name}\")\n",
    "    return decoder\n",
    "\n",
    "def create_segmenter(model_cfg):\n",
    "    model_cfg = model_cfg.copy()\n",
    "    decoder_cfg = model_cfg.pop(\"decoder\")\n",
    "    decoder_cfg[\"n_cls\"] = model_cfg[\"n_cls\"]\n",
    "\n",
    "    encoder = create_vit(model_cfg)\n",
    "    decoder = create_decoder(encoder, decoder_cfg)\n",
    "    model = Segmenter(encoder, decoder, n_cls=model_cfg[\"n_cls\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_model(model_path):\n",
    "    variant_path = Path(model_path).parent / \"variant.yml\"\n",
    "    with open(variant_path, \"r\") as f:\n",
    "        variant = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    net_kwargs = variant[\"net_kwargs\"]\n",
    "\n",
    "    model = create_segmenter(net_kwargs)\n",
    "    data = torch.load(model_path, map_location=device)\n",
    "    checkpoint = data[\"model\"]\n",
    "\n",
    "    model.load_state_dict(checkpoint, strict=True)\n",
    "    return model, variant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8df451",
   "metadata": {
    "papermill": {
     "duration": 0.0082,
     "end_time": "2024-07-18T00:24:34.231580",
     "exception": false,
     "start_time": "2024-07-18T00:24:34.223380",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Darwin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dae904f",
   "metadata": {
    "papermill": {
     "duration": 0.008223,
     "end_time": "2024-07-18T00:24:34.248545",
     "exception": false,
     "start_time": "2024-07-18T00:24:34.240322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eba7afa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T00:24:34.267437Z",
     "iopub.status.busy": "2024-07-18T00:24:34.267072Z",
     "iopub.status.idle": "2024-07-18T00:24:36.551571Z",
     "shell.execute_reply": "2024-07-18T00:24:36.550476Z"
    },
    "papermill": {
     "duration": 2.297233,
     "end_time": "2024-07-18T00:24:36.554388",
     "exception": false,
     "start_time": "2024-07-18T00:24:34.257155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.datasets.flickr import glob\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SegImageDataset(Dataset):\n",
    "    def __init__(self, imgs, masks):\n",
    "        self.imgs = imgs\n",
    "        self.masks = masks\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = read_image(self.imgs[idx]).float() / 255.0\n",
    "        mask = read_image(self.masks[idx], ImageReadMode.GRAY_ALPHA).float() / 255.0\n",
    "        # binary_mask = torch.where(masked < 0.5, torch.tensor(0), torch.tensor(1))\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ba0903",
   "metadata": {
    "papermill": {
     "duration": 0.00826,
     "end_time": "2024-07-18T00:24:36.571654",
     "exception": false,
     "start_time": "2024-07-18T00:24:36.563394",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create model, loss function, optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "459352bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T00:24:36.591386Z",
     "iopub.status.busy": "2024-07-18T00:24:36.590759Z",
     "iopub.status.idle": "2024-07-18T00:24:39.354990Z",
     "shell.execute_reply": "2024-07-18T00:24:39.354090Z"
    },
    "papermill": {
     "duration": 2.777218,
     "end_time": "2024-07-18T00:24:39.357618",
     "exception": false,
     "start_time": "2024-07-18T00:24:36.580400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import pandas as pd\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryF1Score, BinaryJaccardIndex, BinaryPrecision, BinaryRecall\n",
    "from torchmetrics.segmentation import MeanIoU\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "\n",
    "class SegDM(L.LightningDataModule):\n",
    "    def __init__(self, batch_size, img_dir, mask_dir):\n",
    "        super().__init__()\n",
    "        # read and sort images.\n",
    "        self.image_paths = sorted(list(paths.list_images(img_dir)))\n",
    "        self.mask_paths = sorted(list(paths.list_images(mask_dir)))\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def setup(self, stage: str):\n",
    "        fit_imgs, test_imgs, fit_masks, test_masks = train_test_split(self.image_paths, self.mask_paths, test_size=0.25, random_state=42)\n",
    "        if stage == \"fit\":\n",
    "            train_imgs, val_imgs, train_masks, val_masks = train_test_split(fit_imgs, fit_masks, test_size=0.25, random_state=42)\n",
    "            self.train = SegImageDataset(imgs=train_imgs, masks=train_masks)\n",
    "            self.val = SegImageDataset(imgs=val_imgs, masks=val_masks)\n",
    "            print(f\"{len(self.train)} examples in the training set...\")\n",
    "            print(f\"{len(self.val)} examples in the validation set...\")\n",
    "        if stage == \"test\":\n",
    "            self.test = SegImageDataset(imgs=test_imgs, masks=test_masks)\n",
    "            print(f\"{len(self.test)} examples in the test set...\")\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, shuffle=True, batch_size=self.batch_size, num_workers=4)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, shuffle=False, batch_size=self.batch_size, num_workers=4)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, shuffle=False, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "class SegmenterModule(L.LightningModule):\n",
    "    def __init__(self, config, num_classes=1, learning_rate=0.001):\n",
    "        super().__init__()\n",
    "        \n",
    "        encoder = VisionTransformer(**config['encoder'], n_cls=1000)\n",
    "        dim = encoder.d_model\n",
    "        decoder_cfg = config['decoder']\n",
    "        decoder_cfg[\"d_encoder\"] = encoder.d_model\n",
    "        decoder_cfg[\"n_heads\"] = n_heads\n",
    "        decoder_cfg[\"d_model\"] = dim\n",
    "        decoder_cfg[\"d_ff\"] = 4 * dim\n",
    "        decoder_cfg[\"patch_size\"] = encoder.patch_size\n",
    "\n",
    "        decoder = MaskTransformer(**decoder_cfg, n_cls=num_classes)\n",
    "        \n",
    "        self.model = Segmenter(encoder, decoder, num_classes)\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        self.lr = learning_rate\n",
    "       \n",
    "        self.test_metrics = {\n",
    "            'f1': [], 'accuracy': [], 'precision': [], 'recall': [], 'mean_iou': []\n",
    "        }\n",
    "        self.accuracy = BinaryAccuracy()\n",
    "        self.f1 = BinaryF1Score() \n",
    "        self.precision = BinaryPrecision()\n",
    "        self.recall = BinaryRecall()\n",
    "        self.mean_iou = MeanIoU(num_classes=num_classes)\n",
    "        \n",
    "        # self.train_loss = []\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    def _common_step(self, batch):\n",
    "        imgs, masks = batch\n",
    "        preds = self.model(imgs)\n",
    "        loss = self.loss_fn(preds, masks)\n",
    "        return loss, preds, masks\n",
    "    \n",
    "#     def on_train_epoch_end(self):\n",
    "#         # do something with all training_step outputs, for example:\n",
    "#         epoch_mean = torch.stack(self.train_loss).mean()\n",
    "#         print(\"training_epoch_mean\", epoch_mean)\n",
    "#         self.log(\"training_epoch_mean\", epoch_mean)\n",
    "#         # free up the memory\n",
    "#         self.train_loss.clear()\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        loss, _, _ = self._common_step(batch)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        loss, _, _ = self._common_step(batch)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch):\n",
    "        loss, preds, masks = self._common_step(batch)\n",
    "        self.log('test_loss', loss)\n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "        masks = torch.argmax(masks, dim=1)\n",
    "        self.test_metrics['f1'].append(self.f1(preds, masks).item())\n",
    "        self.test_metrics['accuracy'].append(self.accuracy(preds, masks).item())\n",
    "        self.test_metrics['precision'].append(self.precision(preds, masks).item())\n",
    "        self.test_metrics['recall'].append(self.recall(preds, masks).item())\n",
    "        self.test_metrics['mean_iou'].append(self.mean_iou(preds, masks).item())\n",
    "        return loss\n",
    "\n",
    "    def on_test_end(self):\n",
    "        df = pd.DataFrame(self.test_metrics)\n",
    "        df.to_csv('test_result.csv', index=None)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93d86c2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T00:24:39.376596Z",
     "iopub.status.busy": "2024-07-18T00:24:39.376228Z",
     "iopub.status.idle": "2024-07-18T00:24:39.381589Z",
     "shell.execute_reply": "2024-07-18T00:24:39.380417Z"
    },
    "papermill": {
     "duration": 0.01725,
     "end_time": "2024-07-18T00:24:39.383733",
     "exception": false,
     "start_time": "2024-07-18T00:24:39.366483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# darwin_config = {\n",
    "#     'encoder': {\n",
    "#         'image_size': (512, 512),\n",
    "#         'patch_size': 16,\n",
    "#         'd_model': 192,\n",
    "#         'n_heads': 8,\n",
    "#         'd_ff': 128,\n",
    "#         'n_layers': 12,\n",
    "#         'distilled': False,\n",
    "#         'channels': 3,\n",
    "#     },\n",
    "#     'decoder': {\n",
    "#         'drop_path_rate': 0.0,\n",
    "#         'dropout': 0.1,\n",
    "#         'n_layers': 2,\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# dm = SegDM(\n",
    "#     batch_size=4,\n",
    "#     mask_dir='/kaggle/input/img-segmentation/Darwin/mask',\n",
    "#     img_dir='/kaggle/input/img-segmentation/Darwin/img',\n",
    "# )\n",
    "# logger = CSVLogger(\"logs\", name=f\"darwin\", flush_logs_every_n_steps=1)\n",
    "# model = SegmenterModule(darwin_config, num_classes=2)\n",
    "# trainer = L.Trainer(fast_dev_run=False, logger=logger, max_time=\"00:11:00:00\")\n",
    "# trainer.fit(model, dm)\n",
    "# trainer.test(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfd4c1b",
   "metadata": {
    "papermill": {
     "duration": 0.008243,
     "end_time": "2024-07-18T00:24:39.400559",
     "exception": false,
     "start_time": "2024-07-18T00:24:39.392316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Shenzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6bb0ec6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T00:24:39.419562Z",
     "iopub.status.busy": "2024-07-18T00:24:39.419186Z",
     "iopub.status.idle": "2024-07-18T00:24:52.650533Z",
     "shell.execute_reply": "2024-07-18T00:24:52.649123Z"
    },
    "papermill": {
     "duration": 13.243906,
     "end_time": "2024-07-18T00:24:52.653219",
     "exception": false,
     "start_time": "2024-07-18T00:24:39.409313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: GPU available: False, used: False\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318 examples in the training set...\n",
      "106 examples in the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: \n",
      "  | Name      | Type              | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | model     | Segmenter         | 3.9 M  | train\n",
      "1 | loss_fn   | BCEWithLogitsLoss | 0      | train\n",
      "2 | accuracy  | BinaryAccuracy    | 0      | train\n",
      "3 | f1        | BinaryF1Score     | 0      | train\n",
      "4 | precision | BinaryPrecision   | 0      | train\n",
      "5 | recall    | BinaryRecall      | 0      | train\n",
      "6 | mean_iou  | MeanIoU           | 0      | train\n",
      "--------------------------------------------------------\n",
      "3.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.9 M     Total params\n",
      "15.684    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7857b779574423a052d1076ea912e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9031b16678e444b69a96bc5f3289964a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142 examples in the test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20bd1d31fc2b4456ba8df6989a5db8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2932112216949463     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2932112216949463    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.2932112216949463}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shenzen_config = {\n",
    "    'encoder': {\n",
    "        'image_size': (512, 512),\n",
    "        'patch_size': 16,\n",
    "        'd_model': 192,\n",
    "        'n_heads': 8,\n",
    "        'd_ff': 128,\n",
    "        'n_layers': 12,\n",
    "        'distilled': False,\n",
    "        'channels': 3,\n",
    "    },\n",
    "    'decoder': {\n",
    "        'drop_path_rate': 0.0,\n",
    "        'dropout': 0.1,\n",
    "        'n_layers': 2,\n",
    "    },\n",
    "}\n",
    "\n",
    "shenzen_dm = SegDM(\n",
    "    batch_size=2,\n",
    "    mask_dir='/kaggle/input/img-segmentation/Shenzhen/mask',\n",
    "    img_dir='/kaggle/input/img-segmentation/Shenzhen/img',\n",
    ")\n",
    "shenzen_logger = CSVLogger(\"logs\", name=f\"shenzen\", flush_logs_every_n_steps=1)\n",
    "# shenzen_model = SegmenterModule(shenzen_config, num_classes=2)\n",
    "shenzen_model = SegmenterModule.load_from_checkpoint('/kaggle/input/segmenter/pytorch/shenzen-e9/2/shenzen-epoch9-step1590.ckpt')\n",
    "shenzen_trainer = L.Trainer(fast_dev_run=True, logger=shenzen_logger, max_epochs=10)\n",
    "shenzen_trainer.fit(shenzen_model, shenzen_dm)\n",
    "shenzen_trainer.test(shenzen_model, shenzen_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1338952",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T00:24:52.676178Z",
     "iopub.status.busy": "2024-07-18T00:24:52.675141Z",
     "iopub.status.idle": "2024-07-18T00:24:52.680788Z",
     "shell.execute_reply": "2024-07-18T00:24:52.679763Z"
    },
    "papermill": {
     "duration": 0.019418,
     "end_time": "2024-07-18T00:24:52.682901",
     "exception": false,
     "start_time": "2024-07-18T00:24:52.663483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# shenzen_dm = SegDM(\n",
    "#     batch_size=4,\n",
    "#     mask_dir='/kaggle/input/img-segmentation/Shenzhen/mask',\n",
    "#     img_dir='/kaggle/input/img-segmentation/Shenzhen/img',\n",
    "# )\n",
    "# shenzen_dm.setup('test')\n",
    "\n",
    "# loader = shenzen_dm.test_dataloader()\n",
    "# test_imgs, test_masks = next(iter(loader))\n",
    "# print(test_imgs.shape)\n",
    "# loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "# trained_shenzen = SegmenterModule.load_from_checkpoint(\"/kaggle/input/segmenter/pytorch/shenzen-e9/2/shenzen-epoch9-step1590.ckpt\")\n",
    "# trained_shenzen.freeze()\n",
    "\n",
    "# pred_masks = trained_shenzen.model(test_imgs)\n",
    "\n",
    "# print('loss ', loss(pred_masks[1], test_masks[1]))\n",
    "# pred_mask = torch.argmax(pred_masks[1], dim=0)\n",
    "# test_mask = torch.argmax(test_masks[1], dim=0)\n",
    "\n",
    "# print(pred_masks[1])\n",
    "# print(test_masks[1])\n",
    "# plt.imshow(pred_mask, cmap='gray')\n",
    "# plt.imshow(test_mask, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00b2286c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T00:24:52.704523Z",
     "iopub.status.busy": "2024-07-18T00:24:52.703759Z",
     "iopub.status.idle": "2024-07-18T00:24:52.708426Z",
     "shell.execute_reply": "2024-07-18T00:24:52.707313Z"
    },
    "papermill": {
     "duration": 0.017909,
     "end_time": "2024-07-18T00:24:52.710702",
     "exception": false,
     "start_time": "2024-07-18T00:24:52.692793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create a random RGB image\n",
    "# rgb_image = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n",
    "\n",
    "# # Display the RGB image\n",
    "# plt.imshow(rgb_image)\n",
    "# plt.title('Random RGB Image')\n",
    "# plt.axis('off')  # Hide the axis\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4c9de3",
   "metadata": {
    "papermill": {
     "duration": 0.009511,
     "end_time": "2024-07-18T00:24:52.730115",
     "exception": false,
     "start_time": "2024-07-18T00:24:52.720604",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca4cca61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T00:24:52.751647Z",
     "iopub.status.busy": "2024-07-18T00:24:52.751239Z",
     "iopub.status.idle": "2024-07-18T00:24:52.757060Z",
     "shell.execute_reply": "2024-07-18T00:24:52.755890Z"
    },
    "papermill": {
     "duration": 0.019178,
     "end_time": "2024-07-18T00:24:52.759318",
     "exception": false,
     "start_time": "2024-07-18T00:24:52.740140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# covid_config = {\n",
    "#     'encoder': {\n",
    "#         'image_size': (299, 299),\n",
    "#         'patch_size': 23,\n",
    "#         'd_model': 192,\n",
    "#         'n_heads': 8,\n",
    "#         'd_ff': 128,\n",
    "#         'n_layers': 12,\n",
    "#         'distilled': False,\n",
    "#         'channels': 1,\n",
    "#     },\n",
    "#     'decoder': {\n",
    "#         'drop_path_rate': 0.0,\n",
    "#         'dropout': 0.1,\n",
    "#         'n_layers': 2,\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# covid_dm = SegDM(\n",
    "#     batch_size=4,\n",
    "#     img_dir='/kaggle/input/img-segmentation/Covid19 Radiography/COVID-19_Radiography_Dataset/COVID/images',\n",
    "#     mask_dir='/kaggle/input/img-segmentation/Covid19 Radiography/COVID-19_Radiography_Dataset/COVID/masks',\n",
    "# )\n",
    "# covid_logger = CSVLogger(\"logs\", name=f\"covid\", flush_logs_every_n_steps=1)\n",
    "# covid_model = SegmenterModule(covid_config, num_classes=2)\n",
    "# covid_trainer = L.Trainer(fast_dev_run=True, logger=covid_logger, max_epochs=10)\n",
    "# covid_trainer.fit(covid_model, covid_dm)\n",
    "# covid_trainer.test(covid_model, covid_dm)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5389689,
     "sourceId": 8977681,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 65733,
     "sourceId": 78676,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 48.07248,
   "end_time": "2024-07-18T00:24:55.204135",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-18T00:24:07.131655",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0233d3777b2a4e9bba9ca04873f73f0a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "08a0581f3c294b96823a400e76e3c602": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b8a046519a904b5d8b95f26414f4d3b6",
       "placeholder": "​",
       "style": "IPY_MODEL_960edd246d5d446eb73fd61dcb1affe5",
       "value": "Epoch 0: 100%"
      }
     },
     "0bf92459f203490f8a08c433ce3716c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e1cdbed474aa41da8e7eb552bae9ee07",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0233d3777b2a4e9bba9ca04873f73f0a",
       "value": 1.0
      }
     },
     "12de64270a9d48b1b0265e8f9d0dbce0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": "hidden",
       "width": "100%"
      }
     },
     "19a76570cfc747e993ac158453e6ad4a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1ea1b82ad4ff40319df7ad81773f7d20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_207da30276c9416182992b3be6cde9a2",
       "placeholder": "​",
       "style": "IPY_MODEL_40c1bfff2e664b358ee21e76454a59be",
       "value": " 1/1 [00:02&lt;00:00,  0.49it/s]"
      }
     },
     "207da30276c9416182992b3be6cde9a2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "20bd1d31fc2b4456ba8df6989a5db8d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_799b3e790ad24b06b36479f91c3852d6",
        "IPY_MODEL_2e4b419e8dcd4ea38fb2f0350f129c58",
        "IPY_MODEL_1ea1b82ad4ff40319df7ad81773f7d20"
       ],
       "layout": "IPY_MODEL_a30c8c7f7ac44bcd8d82393da4a0267a"
      }
     },
     "2e4b419e8dcd4ea38fb2f0350f129c58": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_19a76570cfc747e993ac158453e6ad4a",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ad70fc90e3ee472ab4d0efb97481232b",
       "value": 1.0
      }
     },
     "320e6c84b6224f7880c5b26e98c438db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7f16b14face348d0ac59932cbbb78a72",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_491ead4c63d44a0f928277996d1a7f4d",
       "value": 1.0
      }
     },
     "3edbe993f2964c72ad5baa1e2212e59e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a25451e6a5174b3597f10944671f1212",
       "placeholder": "​",
       "style": "IPY_MODEL_b5fdcd593d244811b46e25e7bbc87154",
       "value": " 1/1 [00:01&lt;00:00,  0.54it/s]"
      }
     },
     "40c1bfff2e664b358ee21e76454a59be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "491ead4c63d44a0f928277996d1a7f4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4f7857b779574423a052d1076ea912e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_08a0581f3c294b96823a400e76e3c602",
        "IPY_MODEL_320e6c84b6224f7880c5b26e98c438db",
        "IPY_MODEL_d5753f647cfa4a4795ccdca301c9393c"
       ],
       "layout": "IPY_MODEL_951af158c56c4822ac8f295e17f45202"
      }
     },
     "6bf74143746243179a15a2cb3f51fcce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7a7ede6463f4458fbdc51f20d3931adc",
       "placeholder": "​",
       "style": "IPY_MODEL_a55c8eac3c4c4f38a5205aca5406a77c",
       "value": "Validation DataLoader 0: 100%"
      }
     },
     "799b3e790ad24b06b36479f91c3852d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_900d89f411a8483cb7f1695a5c8c7976",
       "placeholder": "​",
       "style": "IPY_MODEL_9b3a4746bb5446e2a0eb9df936a436a3",
       "value": "Testing DataLoader 0: 100%"
      }
     },
     "7a7ede6463f4458fbdc51f20d3931adc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7f16b14face348d0ac59932cbbb78a72": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "900d89f411a8483cb7f1695a5c8c7976": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9031b16678e444b69a96bc5f3289964a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6bf74143746243179a15a2cb3f51fcce",
        "IPY_MODEL_0bf92459f203490f8a08c433ce3716c8",
        "IPY_MODEL_3edbe993f2964c72ad5baa1e2212e59e"
       ],
       "layout": "IPY_MODEL_12de64270a9d48b1b0265e8f9d0dbce0"
      }
     },
     "951af158c56c4822ac8f295e17f45202": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "960edd246d5d446eb73fd61dcb1affe5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9b3a4746bb5446e2a0eb9df936a436a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a25451e6a5174b3597f10944671f1212": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a30c8c7f7ac44bcd8d82393da4a0267a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "a55c8eac3c4c4f38a5205aca5406a77c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ad70fc90e3ee472ab4d0efb97481232b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b1fae09c241d4e5fba1ef84341590a1c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b5fdcd593d244811b46e25e7bbc87154": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b8a046519a904b5d8b95f26414f4d3b6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d5753f647cfa4a4795ccdca301c9393c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b1fae09c241d4e5fba1ef84341590a1c",
       "placeholder": "​",
       "style": "IPY_MODEL_dcfeca700aeb44aaaff231054d0ee0f2",
       "value": " 1/1 [00:08&lt;00:00,  0.11it/s]"
      }
     },
     "dcfeca700aeb44aaaff231054d0ee0f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e1cdbed474aa41da8e7eb552bae9ee07": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
