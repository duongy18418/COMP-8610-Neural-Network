{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmenter\n",
    "Author Paper: https://arxiv.org/abs/2105.05633"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (0.8.0)\n",
      "Requirement already satisfied: timm in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (1.0.7)\n",
      "Requirement already satisfied: torchvision in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from timm) (0.18.1)\n",
      "Requirement already satisfied: huggingface_hub in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from timm) (0.23.4)\n",
      "Requirement already satisfied: safetensors in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from timm) (0.4.3)\n",
      "Requirement already satisfied: pyyaml in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: torch in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from timm) (2.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from huggingface_hub->timm) (4.9.0)\n",
      "Requirement already satisfied: requests in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: filelock in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from huggingface_hub->timm) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from huggingface_hub->timm) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from huggingface_hub->timm) (23.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from huggingface_hub->timm) (4.66.4)\n",
      "Requirement already satisfied: sympy in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from torch->timm) (1.12)\n",
      "Requirement already satisfied: jinja2 in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from torch->timm) (3.1.3)\n",
      "Requirement already satisfied: networkx in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from torch->timm) (3.2.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from torchvision->timm) (10.2.0)\n",
      "Requirement already satisfied: numpy in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from torchvision->timm) (1.24.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from jinja2->torch->timm) (2.1.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2024.6.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/haily/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.1.1 is available.\n",
      "You should consider upgrading via the '/Users/haily/.pyenv/versions/3.10.4/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install einops timm imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmenter implemtation\n",
    "Implementation is adopted from https://github.com/rstrudel/segmenter/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from 2020 Ross Wightman\n",
    "https://github.com/rwightman/pytorch-image-models\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import DropPath\n",
    "import torch\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout, out_dim=None):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        if out_dim is None:\n",
    "            out_dim = dim\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    @property\n",
    "    def unwrapped(self):\n",
    "        return self\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        head_dim = dim // heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.attn = None\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "\n",
    "    @property\n",
    "    def unwrapped(self):\n",
    "        return self\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, N, C = x.shape\n",
    "        qkv = (\n",
    "            self.qkv(x)\n",
    "            .reshape(B, N, 3, self.heads, C // self.heads)\n",
    "            .permute(2, 0, 3, 1, 4)\n",
    "        )\n",
    "        \n",
    "        q, k, v = (\n",
    "            qkv[0],\n",
    "            qkv[1],\n",
    "            qkv[2],\n",
    "        )\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "       \n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "    \n",
    "        return x, attn\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_dim, dropout, drop_path):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, heads, dropout)\n",
    "        self.mlp = FeedForward(dim, mlp_dim, dropout)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        y, attn = self.attn(x, mask)\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from timm.models.layers import trunc_normal_\n",
    "\n",
    "##### outer folder utility\n",
    "import os\n",
    "import torch\n",
    "\"\"\"\n",
    "GPU wrappers\n",
    "\"\"\"\n",
    "\n",
    "use_gpu = False\n",
    "gpu_id = 0\n",
    "device = None\n",
    "\n",
    "distributed = False\n",
    "dist_rank = 0\n",
    "world_size = 1\n",
    "\n",
    "\n",
    "def set_gpu_mode(mode):\n",
    "    global use_gpu\n",
    "    global device\n",
    "    global gpu_id\n",
    "    global distributed\n",
    "    global dist_rank\n",
    "    global world_size\n",
    "    gpu_id = int(os.environ.get(\"SLURM_LOCALID\", 0))\n",
    "    dist_rank = int(os.environ.get(\"SLURM_PROCID\", 0))\n",
    "    world_size = int(os.environ.get(\"SLURM_NTASKS\", 1))\n",
    "\n",
    "    distributed = world_size > 1\n",
    "    use_gpu = mode\n",
    "    device = torch.device(f\"cuda:{gpu_id}\" if use_gpu else \"cpu\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        trunc_normal_(m.weight, std=0.02)\n",
    "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "        nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "\n",
    "def resize_pos_embed(posemb, grid_old_shape, grid_new_shape, num_extra_tokens):\n",
    "    # Rescale the grid of position embeddings when loading from state_dict. Adapted from\n",
    "    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224\n",
    "    posemb_tok, posemb_grid = (\n",
    "        posemb[:, :num_extra_tokens],\n",
    "        posemb[0, num_extra_tokens:],\n",
    "    )\n",
    "    if grid_old_shape is None:\n",
    "        gs_old_h = int(math.sqrt(len(posemb_grid)))\n",
    "        gs_old_w = gs_old_h\n",
    "    else:\n",
    "        gs_old_h, gs_old_w = grid_old_shape\n",
    "\n",
    "    gs_h, gs_w = grid_new_shape\n",
    "    posemb_grid = posemb_grid.reshape(1, gs_old_h, gs_old_w, -1).permute(0, 3, 1, 2)\n",
    "    posemb_grid = F.interpolate(posemb_grid, size=(gs_h, gs_w), mode=\"bilinear\")\n",
    "    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, gs_h * gs_w, -1)\n",
    "    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)\n",
    "    return posemb\n",
    "\n",
    "\n",
    "def checkpoint_filter_fn(state_dict, model):\n",
    "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
    "    out_dict = {}\n",
    "    if \"model\" in state_dict:\n",
    "        # For deit models\n",
    "        state_dict = state_dict[\"model\"]\n",
    "    num_extra_tokens = 1 + (\"dist_token\" in state_dict.keys())\n",
    "    patch_size = model.patch_size\n",
    "    image_size = model.patch_embed.image_size\n",
    "    for k, v in state_dict.items():\n",
    "        if k == \"pos_embed\" and v.shape != model.pos_embed.shape:\n",
    "            # To resize pos embedding when using model at different size from pretrained weights\n",
    "            v = resize_pos_embed(\n",
    "                v,\n",
    "                None,\n",
    "                (image_size[0] // patch_size, image_size[1] // patch_size),\n",
    "                num_extra_tokens,\n",
    "            )\n",
    "        out_dict[k] = v\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "def padding(im, patch_size, fill_value=0):\n",
    "    # make the image sizes divisible by patch_size\n",
    "    H, W = im.size(2), im.size(3)\n",
    "    pad_h, pad_w = 0, 0\n",
    "    if H % patch_size > 0:\n",
    "        pad_h = patch_size - (H % patch_size)\n",
    "    if W % patch_size > 0:\n",
    "        pad_w = patch_size - (W % patch_size)\n",
    "    im_padded = im\n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        im_padded = F.pad(im, (0, pad_w, 0, pad_h), value=fill_value)\n",
    "    return im_padded\n",
    "\n",
    "\n",
    "def unpadding(y, target_size):\n",
    "    H, W = target_size\n",
    "    H_pad, W_pad = y.size(2), y.size(3)\n",
    "    # crop predictions on extra pixels coming from padding\n",
    "    extra_h = H_pad - H\n",
    "    extra_w = W_pad - W\n",
    "    if extra_h > 0:\n",
    "        y = y[:, :, :-extra_h]\n",
    "    if extra_w > 0:\n",
    "        y = y[:, :, :, :-extra_w]\n",
    "    return y\n",
    "\n",
    "\n",
    "def resize(im, smaller_size):\n",
    "    h, w = im.shape[2:]\n",
    "    if h < w:\n",
    "        ratio = w / h\n",
    "        h_res, w_res = smaller_size, ratio * smaller_size\n",
    "    else:\n",
    "        ratio = h / w\n",
    "        h_res, w_res = ratio * smaller_size, smaller_size\n",
    "    if min(h, w) < smaller_size:\n",
    "        im_res = F.interpolate(im, (int(h_res), int(w_res)), mode=\"bilinear\")\n",
    "    else:\n",
    "        im_res = im\n",
    "    return im_res\n",
    "\n",
    "\n",
    "def sliding_window(im, flip, window_size, window_stride):\n",
    "    B, C, H, W = im.shape\n",
    "    ws = window_size\n",
    "\n",
    "    windows = {\"crop\": [], \"anchors\": []}\n",
    "    h_anchors = torch.arange(0, H, window_stride)\n",
    "    w_anchors = torch.arange(0, W, window_stride)\n",
    "    h_anchors = [h.item() for h in h_anchors if h < H - ws] + [H - ws]\n",
    "    w_anchors = [w.item() for w in w_anchors if w < W - ws] + [W - ws]\n",
    "    for ha in h_anchors:\n",
    "        for wa in w_anchors:\n",
    "            window = im[:, :, ha : ha + ws, wa : wa + ws]\n",
    "            windows[\"crop\"].append(window)\n",
    "            windows[\"anchors\"].append((ha, wa))\n",
    "    windows[\"flip\"] = flip\n",
    "    windows[\"shape\"] = (H, W)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def merge_windows(windows, window_size, ori_shape):\n",
    "    ws = window_size\n",
    "    im_windows = windows[\"seg_maps\"]\n",
    "    anchors = windows[\"anchors\"]\n",
    "    C = im_windows[0].shape[0]\n",
    "    H, W = windows[\"shape\"]\n",
    "    flip = windows[\"flip\"]\n",
    "\n",
    "    logit = torch.zeros((C, H, W), device=im_windows.device)\n",
    "    count = torch.zeros((1, H, W), device=im_windows.device)\n",
    "    for window, (ha, wa) in zip(im_windows, anchors):\n",
    "        logit[:, ha : ha + ws, wa : wa + ws] += window\n",
    "        count[:, ha : ha + ws, wa : wa + ws] += 1\n",
    "    logit = logit / count\n",
    "    logit = F.interpolate(\n",
    "        logit.unsqueeze(0),\n",
    "        ori_shape,\n",
    "        mode=\"bilinear\",\n",
    "    )[0]\n",
    "    if flip:\n",
    "        logit = torch.flip(logit, (2,))\n",
    "    result = F.softmax(logit, 0)\n",
    "    return result\n",
    "\n",
    "\n",
    "def inference(\n",
    "    model,\n",
    "    ims,\n",
    "    ims_metas,\n",
    "    ori_shape,\n",
    "    window_size,\n",
    "    window_stride,\n",
    "    batch_size,\n",
    "):\n",
    "    C = model.n_cls\n",
    "    seg_map = torch.zeros((C, ori_shape[0], ori_shape[1]), device=device)\n",
    "    for im, im_metas in zip(ims, ims_metas):\n",
    "        im = im.to(device)\n",
    "        im = resize(im, window_size)\n",
    "        flip = im_metas[\"flip\"]\n",
    "        windows = sliding_window(im, flip, window_size, window_stride)\n",
    "        crops = torch.stack(windows.pop(\"crop\"))[:, 0]\n",
    "        B = len(crops)\n",
    "        WB = batch_size\n",
    "        seg_maps = torch.zeros((B, C, window_size, window_size), device=im.device)\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, B, WB):\n",
    "                seg_maps[i : i + WB] = model.forward(crops[i : i + WB])\n",
    "        windows[\"seg_maps\"] = seg_maps\n",
    "        im_seg_map = merge_windows(windows, window_size, ori_shape)\n",
    "        seg_map += im_seg_map\n",
    "    seg_map /= len(ims)\n",
    "    return seg_map\n",
    "\n",
    "\n",
    "def num_params(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    n_params = sum([torch.prod(torch.tensor(p.size())) for p in model_parameters])\n",
    "    return n_params.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from timm.models.layers import trunc_normal_\n",
    "\n",
    "class DecoderLinear(nn.Module):\n",
    "    def __init__(self, n_cls, patch_size, d_encoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_encoder = d_encoder\n",
    "        self.patch_size = patch_size\n",
    "        self.n_cls = n_cls\n",
    "\n",
    "        self.head = nn.Linear(self.d_encoder, n_cls)\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return set()\n",
    "\n",
    "    def forward(self, x, im_size):\n",
    "        H, W = im_size\n",
    "        GS = H // self.patch_size\n",
    "        x = self.head(x)\n",
    "        ## h*w = n\n",
    "        x = rearrange(x, \"b (h w) c -> b c h w\", h=GS)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MaskTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_cls,\n",
    "        patch_size,\n",
    "        d_encoder,\n",
    "        n_layers,\n",
    "        n_heads,\n",
    "        d_model,\n",
    "        d_ff,\n",
    "        drop_path_rate,\n",
    "        dropout,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_encoder = d_encoder # return by decode n x d\n",
    "        self.patch_size = patch_size \n",
    "        self.n_layers = n_layers\n",
    "        self.n_cls = n_cls\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.scale = d_model ** -0.5 # attention\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, n_layers)]\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Block(d_model, n_heads, d_ff, dropout, dpr[i]) for i in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        # initial class embedding randomly. Shape K x D_model. Parameter means learnable\n",
    "        self.cls_emb = nn.Parameter(torch.randn(1, n_cls, d_model))\n",
    "        # Projection: map D of encoder to D model\n",
    "        self.proj_dec = nn.Linear(d_encoder, d_model)\n",
    "\n",
    "        self.proj_patch = nn.Parameter(self.scale * torch.randn(d_model, d_model))\n",
    "        self.proj_classes = nn.Parameter(self.scale * torch.randn(d_model, d_model))\n",
    "\n",
    "        self.decoder_norm = nn.LayerNorm(d_model)\n",
    "        self.mask_norm = nn.LayerNorm(n_cls)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "        trunc_normal_(self.cls_emb, std=0.02)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {\"cls_emb\"}\n",
    "    \n",
    "    def forward(self, x, im_size):\n",
    "        H, W = im_size\n",
    "        GS = H // self.patch_size\n",
    "\n",
    "        x = self.proj_dec(x)\n",
    "        cls_emb = self.cls_emb.expand(x.size(0), -1, -1)\n",
    "        x = torch.cat((x, cls_emb), 1)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        patches, cls_seg_feat = x[:, : -self.n_cls], x[:, -self.n_cls :]\n",
    "        patches = patches @ self.proj_patch\n",
    "        cls_seg_feat = cls_seg_feat @ self.proj_classes\n",
    "\n",
    "        patches = patches / patches.norm(dim=-1, keepdim=True)\n",
    "        cls_seg_feat = cls_seg_feat / cls_seg_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        masks = patches @ cls_seg_feat.transpose(1, 2)\n",
    "        masks = self.mask_norm(masks)\n",
    "        masks = rearrange(masks, \"b (h w) n -> b n h w\", h=int(GS))\n",
    "\n",
    "        return masks\n",
    "\n",
    "    def get_attention_map(self, x, layer_id):\n",
    "        if layer_id >= self.n_layers or layer_id < 0:\n",
    "            raise ValueError(\n",
    "                f\"Provided layer_id: {layer_id} is not valid. 0 <= {layer_id} < {self.n_layers}.\"\n",
    "            )\n",
    "        x = self.proj_dec(x)\n",
    "        cls_emb = self.cls_emb.expand(x.size(0), -1, -1)\n",
    "        x = torch.cat((x, cls_emb), 1)\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            if i < layer_id:\n",
    "                x = blk(x)\n",
    "            else:\n",
    "                return blk(x, return_attention=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after patch emd: torch.Size([4, 64, 128])\n",
      "pos emb shape: torch.Size([1, 65, 128])\n",
      "cls tokens shape: torch.Size([4, 1, 128])\n",
      "after add cls tokens for to the patches: torch.Size([4, 65, 128])\n",
      "pos_embed after resize:  torch.Size([1, 65, 128])\n",
      "==== Multihead Attention Block\n",
      "- Attention Block\n",
      "mapping from B, N, d_model to BxNx3dim  torch.Size([4, 65, 384])\n",
      "reshape from  B,N,3xdim to B,N,3,heads,d_model / heads  torch.Size([4, 65, 3, 8, 16])\n",
      "permute to 3,B,heads,n,d_model/heads torch.Size([3, 4, 8, 65, 16])\n",
      "q,k,v shape  torch.Size([4, 8, 65, 16])\n",
      "after attention x  torch.Size([4, 65, 128])\n",
      "- After Feedforward, Dropout of MA block x:  torch.Size([4, 65, 128])\n",
      "====Back to Fully connected head====\n",
      "Mask decoder input:  torch.Size([4, 65, 128])\n",
      "Finish vision transformer, output n-classes  torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Adapted from 2020 Ross Wightman\n",
    "https://github.com/rwightman/pytorch-image-models\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.models.vision_transformer import _load_weights\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, embed_dim, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        if image_size[0] % patch_size != 0 or image_size[1] % patch_size != 0:\n",
    "            raise ValueError(\"image dimensions must be divisible by the patch size\")\n",
    "        self.grid_size = image_size[0] // patch_size, image_size[1] // patch_size\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            channels, embed_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, im):\n",
    "        B, C, H, W = im.shape\n",
    "        x = self.proj(im).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        patch_size,\n",
    "        n_layers,\n",
    "        d_model,\n",
    "        d_ff,\n",
    "        n_heads,\n",
    "        n_cls,\n",
    "        dropout=0.1,\n",
    "        drop_path_rate=0.0,\n",
    "        distilled=False,\n",
    "        channels=3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            image_size,\n",
    "            patch_size,\n",
    "            d_model,\n",
    "            channels,\n",
    "        )\n",
    "        self.patch_size = patch_size\n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.n_cls = n_cls\n",
    "\n",
    "        # cls and pos tokens\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.distilled = distilled\n",
    "        if self.distilled:\n",
    "            self.dist_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "            self.pos_embed = nn.Parameter(\n",
    "                torch.randn(1, self.patch_embed.num_patches + 2, d_model)\n",
    "            )\n",
    "            self.head_dist = nn.Linear(d_model, n_cls)\n",
    "        else:\n",
    "            self.pos_embed = nn.Parameter(\n",
    "                torch.randn(1, self.patch_embed.num_patches + 1, d_model)\n",
    "            )\n",
    "\n",
    "        # transformer blocks\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, n_layers)]\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Block(d_model, n_heads, d_ff, dropout, dpr[i]) for i in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        # output head\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, n_cls)\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=0.02)\n",
    "        trunc_normal_(self.cls_token, std=0.02)\n",
    "        if self.distilled:\n",
    "            trunc_normal_(self.dist_token, std=0.02)\n",
    "        self.pre_logits = nn.Identity()\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {\"pos_embed\", \"cls_token\", \"dist_token\"}\n",
    "\n",
    "    @torch.jit.ignore()\n",
    "    def load_pretrained(self, checkpoint_path, prefix=\"\"):\n",
    "        _load_weights(self, checkpoint_path, prefix)\n",
    "\n",
    "    def forward(self, im, return_features=False):\n",
    "        B, _, H, W = im.shape\n",
    "        PS = self.patch_size\n",
    "\n",
    "        x = self.patch_embed(im)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        if self.distilled:\n",
    "            dist_tokens = self.dist_token.expand(B, -1, -1)\n",
    "            x = torch.cat((cls_tokens, dist_tokens, x), dim=1)\n",
    "        else:\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        pos_embed = self.pos_embed\n",
    "        num_extra_tokens = 1 + self.distilled\n",
    "        if x.shape[1] != pos_embed.shape[1]:\n",
    "            pos_embed = resize_pos_embed(\n",
    "                pos_embed,\n",
    "                self.patch_embed.grid_size,\n",
    "                (H // PS, W // PS),\n",
    "                num_extra_tokens,\n",
    "            )\n",
    "        x = x + pos_embed\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if return_features:\n",
    "            return x\n",
    "\n",
    "        if self.distilled:\n",
    "            x, x_dist = x[:, 0], x[:, 1]\n",
    "            x = self.head(x)\n",
    "            x_dist = self.head_dist(x_dist)\n",
    "            x = (x + x_dist) / 2\n",
    "        else:\n",
    "            x = x[:, 0]\n",
    "            x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def get_attention_map(self, im, layer_id):\n",
    "        if layer_id >= self.n_layers or layer_id < 0:\n",
    "            raise ValueError(\n",
    "                f\"Provided layer_id: {layer_id} is not valid. 0 <= {layer_id} < {self.n_layers}.\"\n",
    "            )\n",
    "        B, _, H, W = im.shape\n",
    "        PS = self.patch_size\n",
    "\n",
    "        x = self.patch_embed(im)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        if self.distilled:\n",
    "            dist_tokens = self.dist_token.expand(B, -1, -1)\n",
    "            x = torch.cat((cls_tokens, dist_tokens, x), dim=1)\n",
    "        else:\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        pos_embed = self.pos_embed\n",
    "        num_extra_tokens = 1 + self.distilled\n",
    "        if x.shape[1] != pos_embed.shape[1]:\n",
    "            pos_embed = resize_pos_embed(\n",
    "                pos_embed,\n",
    "                self.patch_embed.grid_size,\n",
    "                (H // PS, W // PS),\n",
    "                num_extra_tokens,\n",
    "            )\n",
    "        x = x + pos_embed\n",
    "\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            if i < layer_id:\n",
    "                x = blk(x)\n",
    "            else:\n",
    "                return blk(x, return_attention=True)\n",
    "\n",
    "\n",
    "d_model = 128\n",
    "batch = 4\n",
    "channels = 3\n",
    "img_size = (320, 320)\n",
    "patch_size = 40\n",
    "drop_path_rate = .5\n",
    "n_layers = 1\n",
    "n_heads = 8\n",
    "d_ff = 128\n",
    "\n",
    "n_cls = 10\n",
    "pe = PatchEmbedding(img_size, patch_size, d_model, channels)\n",
    "distilled = False\n",
    "x = torch.rand(batch, channels, img_size[0], img_size[1])\n",
    "x = pe(x)\n",
    "print(\"after patch emd:\", x.shape)\n",
    "\n",
    "pos_embed = nn.Parameter(torch.randn(1, pe.num_patches + 1, d_model))\n",
    "print(\"pos emb shape:\", pos_embed.size())\n",
    "\n",
    "cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "cls_tokens = cls_token.expand(batch, -1, -1)\n",
    "print(\"cls tokens shape:\", cls_tokens.shape)\n",
    "\n",
    "cls_token.expand(batch, -1, -1)\n",
    "x = torch.cat((cls_tokens, x), dim=1) # This is why pos_emd has the second dimension is num_patches + 1.. We added cls_tokens \n",
    "\n",
    "print(\"after add cls tokens for to the patches:\", x.shape)\n",
    "\n",
    "dropout = nn.Dropout(0.5)\n",
    "\n",
    "num_extra_tokens = 1 + distilled\n",
    "if x.shape[1] != pos_embed.shape[1]:\n",
    "    pos_embed = resize_pos_embed(\n",
    "        pos_embed,\n",
    "        pe.grid_size,\n",
    "        (img_size[0] // patch_size, img_size[1] // patch_size),\n",
    "        num_extra_tokens,\n",
    "    )\n",
    "print('pos_embed after resize: ', pos_embed.shape)\n",
    "\n",
    "x = x + pos_embed\n",
    "x = dropout(x)\n",
    "\n",
    "dropout = 0.5\n",
    "dpr = [x.item() for x in torch.linspace(0, drop_path_rate, n_layers)]\n",
    "\n",
    "print(\"==== Multihead Attention Block\")\n",
    "dim = d_model\n",
    "\n",
    "norm1 = nn.LayerNorm(dim)\n",
    "x = norm1(x)\n",
    "\n",
    "y = torch.Tensor(x)\n",
    "heads = n_heads\n",
    "head_dim = dim // heads\n",
    "scale = head_dim ** -0.5\n",
    "attn = None\n",
    "\n",
    "## Attention\n",
    "print(\"- Attention Block\")\n",
    "qkv = nn.Linear(dim, dim * 3)\n",
    "attn_drop = nn.Dropout(dropout)\n",
    "proj = nn.Linear(dim, dim)\n",
    "proj_drop = nn.Dropout(dropout)\n",
    "\n",
    "B, N, C = x.shape\n",
    "\n",
    "projected = qkv(y)\n",
    "print(\"mapping from B, N, d_model to BxNx3dim \", projected.shape)\n",
    "projected = projected.reshape(B, N, 3, heads, C // heads)\n",
    "\n",
    "# n_heads used hear, to divide d_model to 2 dimensions: heads and d_model / heads => Multihead attention ?\n",
    "print(\"reshape from  B,N,3xdim to B,N,3,heads,d_model / heads \", projected.shape)\n",
    "projected = projected.permute(2, 0, 3, 1, 4)\n",
    "print(\"permute to 3,B,heads,n,d_model/heads\" , projected.shape)\n",
    "q, k, v = projected[0], projected[1], projected[2]\n",
    "print(\"q,k,v shape \", q.shape)\n",
    "\n",
    "attn = (q @ k.transpose(-2, -1)) * scale\n",
    "attn = attn.softmax(dim=-1)\n",
    "attn = attn_drop(attn)\n",
    "\n",
    "y = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "y = proj(y)\n",
    "y = proj_drop(y)\n",
    "print(\"after attention x \", y.shape)\n",
    "\n",
    "# back to block\n",
    "norm2 = nn.LayerNorm(dim)\n",
    "mlp = FeedForward(dim, d_ff, dropout)\n",
    "drop_path = DropPath(0.1) if 0.1 > 0.0 else nn.Identity()\n",
    "\n",
    "x = x + drop_path(y)\n",
    "x = x + drop_path(mlp(norm2(x)))\n",
    "\n",
    "print('- After Feedforward, Dropout of MA block x: ', x.shape)\n",
    "\n",
    "norm = nn.LayerNorm(d_model)\n",
    "head = nn.Linear(d_model, n_cls)\n",
    "\n",
    "print('====Back to Fully connected head====')\n",
    "x = norm(x)\n",
    "print(\"Mask decoder input: \", x.shape)\n",
    "x = x[:, 0]\n",
    "x = head(x)\n",
    "print(\"Finish vision transformer, output n-classes \", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmenter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import trunc_normal_\n",
    "\n",
    "class Segmenter(nn.Module):\n",
    "    def __init__(self, encoder, decoder, n_cls):\n",
    "        super().__init__()\n",
    "        self.n_cls = n_cls\n",
    "        self.patch_size = encoder.patch_size\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        def append_prefix_no_weight_decay(prefix, module):\n",
    "            return set(map(lambda x: prefix + x, module.no_weight_decay()))\n",
    "\n",
    "        nwd_params = append_prefix_no_weight_decay(\"encoder.\", self.encoder).union(\n",
    "            append_prefix_no_weight_decay(\"decoder.\", self.decoder)\n",
    "        )\n",
    "        return nwd_params\n",
    "\n",
    "    def forward(self, im):\n",
    "        H_ori, W_ori = im.size(2), im.size(3)\n",
    "        im = padding(im, self.patch_size)\n",
    "        H, W = im.size(2), im.size(3)\n",
    "\n",
    "        x = self.encoder(im, return_features=True)\n",
    "\n",
    "        # remove CLS/DIST tokens for decoding\n",
    "        num_extra_tokens = 1 + self.encoder.distilled\n",
    "        x = x[:, num_extra_tokens:]\n",
    "\n",
    "        masks = self.decoder(x, (H, W))\n",
    "\n",
    "        masks = F.interpolate(masks, size=(H, W), mode=\"bilinear\")\n",
    "        masks = unpadding(masks, (H_ori, W_ori))\n",
    "\n",
    "        return masks\n",
    "\n",
    "    def get_attention_map_enc(self, im, layer_id):\n",
    "        return self.encoder.get_attention_map(im, layer_id)\n",
    "\n",
    "    def get_attention_map_dec(self, im, layer_id):\n",
    "        x = self.encoder(im, return_features=True)\n",
    "\n",
    "        # remove CLS/DIST tokens for decoding\n",
    "        num_extra_tokens = 1 + self.encoder.distilled\n",
    "        x = x[:, num_extra_tokens:]\n",
    "\n",
    "        return self.decoder.get_attention_map(x, layer_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "import torch\n",
    "import os\n",
    "from timm.models.helpers import load_pretrained, load_custom_pretrained\n",
    "from timm.models.vision_transformer import default_cfgs\n",
    "from timm.models.registry import register_model\n",
    "from timm.models.vision_transformer import _create_vision_transformer\n",
    "\n",
    "\n",
    "@register_model\n",
    "def vit_base_patch8_384(pretrained=False, **kwargs):\n",
    "    \"\"\"ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.\n",
    "    \"\"\"\n",
    "    model_kwargs = dict(patch_size=8, embed_dim=768, depth=12, num_heads=12, **kwargs)\n",
    "    model = _create_vision_transformer(\n",
    "        \"vit_base_patch8_384\",\n",
    "        pretrained=pretrained,\n",
    "        default_cfg=dict(\n",
    "            url=\"\",\n",
    "            input_size=(3, 384, 384),\n",
    "            mean=(0.5, 0.5, 0.5),\n",
    "            std=(0.5, 0.5, 0.5),\n",
    "            num_classes=1000,\n",
    "        ),\n",
    "        **model_kwargs,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def create_vit(model_cfg):\n",
    "    model_cfg = model_cfg.copy()\n",
    "    backbone = model_cfg.pop(\"backbone\")\n",
    "\n",
    "    normalization = model_cfg.pop(\"normalization\")\n",
    "    model_cfg[\"n_cls\"] = 1000\n",
    "    mlp_expansion_ratio = 4\n",
    "    model_cfg[\"d_ff\"] = mlp_expansion_ratio * model_cfg[\"d_model\"]\n",
    "\n",
    "    if backbone in default_cfgs:\n",
    "        default_cfg = default_cfgs[backbone]\n",
    "    else:\n",
    "        default_cfg = dict(\n",
    "            pretrained=False,\n",
    "            num_classes=1000,\n",
    "            drop_rate=0.0,\n",
    "            drop_path_rate=0.0,\n",
    "            drop_block_rate=None,\n",
    "        )\n",
    "    \n",
    "    default_cfg[\"input_size\"] = (\n",
    "        3,\n",
    "        model_cfg[\"image_size\"][0],\n",
    "        model_cfg[\"image_size\"][1],\n",
    "    )\n",
    "\n",
    "    model = VisionTransformer(**model_cfg)\n",
    "    if backbone == \"vit_base_patch8_384\":\n",
    "        path = os.path.expandvars(\"$TORCH_HOME/hub/checkpoints/vit_base_patch8_384.pth\")\n",
    "        state_dict = torch.load(path, map_location=\"cpu\")\n",
    "        filtered_dict = checkpoint_filter_fn(state_dict, model)\n",
    "        model.load_state_dict(filtered_dict, strict=True)\n",
    "    elif \"deit\" in backbone:\n",
    "        load_pretrained(model, default_cfg, filter_fn=checkpoint_filter_fn)\n",
    "    else:\n",
    "        load_custom_pretrained(model, default_cfg)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def create_decoder(encoder, decoder_cfg):\n",
    "    decoder_cfg = decoder_cfg.copy()\n",
    "    name = decoder_cfg.pop(\"name\")\n",
    "    decoder_cfg[\"d_encoder\"] = encoder.d_model\n",
    "    decoder_cfg[\"patch_size\"] = encoder.patch_size\n",
    "\n",
    "    if \"linear\" in name:\n",
    "        decoder = DecoderLinear(**decoder_cfg)\n",
    "    elif name == \"mask_transformer\":\n",
    "        dim = encoder.d_model\n",
    "        n_heads = dim // 64\n",
    "        decoder_cfg[\"n_heads\"] = n_heads\n",
    "        decoder_cfg[\"d_model\"] = dim\n",
    "        decoder_cfg[\"d_ff\"] = 4 * dim\n",
    "        decoder = MaskTransformer(**decoder_cfg)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown decoder: {name}\")\n",
    "    return decoder\n",
    "\n",
    "def create_segmenter(model_cfg):\n",
    "    model_cfg = model_cfg.copy()\n",
    "    decoder_cfg = model_cfg.pop(\"decoder\")\n",
    "    decoder_cfg[\"n_cls\"] = model_cfg[\"n_cls\"]\n",
    "\n",
    "    encoder = create_vit(model_cfg)\n",
    "    decoder = create_decoder(encoder, decoder_cfg)\n",
    "    model = Segmenter(encoder, decoder, n_cls=model_cfg[\"n_cls\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_model(model_path):\n",
    "    variant_path = Path(model_path).parent / \"variant.yml\"\n",
    "    with open(variant_path, \"r\") as f:\n",
    "        variant = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    net_kwargs = variant[\"net_kwargs\"]\n",
    "\n",
    "    model = create_segmenter(net_kwargs)\n",
    "    data = torch.load(model_path, map_location=device)\n",
    "    checkpoint = data[\"model\"]\n",
    "\n",
    "    model.load_state_dict(checkpoint, strict=True)\n",
    "    return model, variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 512, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]],\n",
       "       grad_fn=<UpsampleBilinear2DBackward0>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darwin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] found 4579 examples in the training set...\n",
      "[INFO] found 1527 examples in the test set...\n"
     ]
    }
   ],
   "source": [
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.datasets.flickr import glob\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SegImageDataset(Dataset):\n",
    "    def __init__(self, imgs, masks):\n",
    "        self.imgs = imgs\n",
    "        self.masks = masks\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = read_image(self.imgs[idx]).float() / 255.0\n",
    "        masked = read_image(self.masks[idx]).float() / 255.0\n",
    "        return image, masked\n",
    "    \n",
    "BATCH_SIZE = 8\n",
    "# read and sort images.\n",
    "image_paths = sorted(list(paths.list_images('./Darwin/img')))\n",
    "mask_paths = sorted(list(paths.list_images('./Darwin/mask')))\n",
    "\n",
    "split = train_test_split(image_paths, mask_paths, test_size=0.25, random_state=42)\n",
    "# unpack the data split\n",
    "(train_imgs, test_imgs) = split[:2]\n",
    "(train_masks, test_masks) = split[2:]\n",
    "\n",
    "# create the train and test datasets\n",
    "darwin_train = SegImageDataset(imgs=train_imgs, masks=train_masks)\n",
    "darwin_test = SegImageDataset(imgs=test_imgs, masks=test_masks)\n",
    "print(f\"[INFO] found {len(darwin_train)} examples in the training set...\")\n",
    "print(f\"[INFO] found {len(darwin_test)} examples in the test set...\")\n",
    "# create the training and test data loaders\n",
    "train_loader = DataLoader(darwin_train, shuffle=True, \n",
    "                         batch_size=BATCH_SIZE, num_workers=0)\n",
    "test_loader = DataLoader(darwin_test, shuffle=False,\n",
    "\tbatch_size=BATCH_SIZE,\n",
    "\tnum_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model, loss function, optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "darwin_config = {\n",
    "    'encoder': {\n",
    "        'image_size': (512, 512),\n",
    "        'patch_size': 16,\n",
    "        'd_model': 768,\n",
    "        'n_heads': 12,\n",
    "        'd_ff': 128,\n",
    "        'n_layers': 12,\n",
    "        'distilled': False,\n",
    "        'channels': 3,\n",
    "    },\n",
    "    'decoder': {\n",
    "        'drop_path_rate': 0.0,\n",
    "        'dropout': 0.1,\n",
    "        'n_layers': 2,\n",
    "    } \n",
    "}\n",
    "\n",
    "INIT_LR = 0.001\n",
    "n_cls = 1\n",
    "encoder = VisionTransformer(**darwin_config['encoder'], n_cls=1000)\n",
    "dim = encoder.d_model\n",
    "decoder_cfg = darwin_config['decoder']\n",
    "decoder_cfg[\"d_encoder\"] = encoder.d_model\n",
    "decoder_cfg[\"n_heads\"] = n_heads\n",
    "decoder_cfg[\"d_model\"] = dim\n",
    "decoder_cfg[\"d_ff\"] = 4 * dim\n",
    "decoder_cfg[\"patch_size\"] = encoder.patch_size\n",
    "\n",
    "decoder = MaskTransformer(**decoder_cfg, n_cls=n_cls)\n",
    "seg = Segmenter(encoder, decoder, n_cls)\n",
    "\n",
    "\n",
    "# initialize loss function and optimizer\n",
    "lossFunc = torch.nn.BCEWithLogitsLoss()\n",
    "opt = torch.optim.Adam(seg.parameters(), lr=INIT_LR)\n",
    "# calculate steps per epoch for training and test set\n",
    "train_steps = len(darwin_train) // BATCH_SIZE\n",
    "test_steps = len(darwin_test) // BATCH_SIZE\n",
    "# initialize a dictionary to store training history\n",
    "H = {\"train_loss\": [], \"test_loss\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training the network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# loop over epochs\n",
    "NUM_EPOCHS = 1\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "for e in tqdm(range(NUM_EPOCHS)):\n",
    "\t# set the model in training mode\n",
    "\tseg.train()\n",
    "\t# initialize the total training and validation loss\n",
    "\ttotalTrainLoss = 0\n",
    "\ttotalTestLoss = 0\n",
    "\t# loop over the training set\n",
    "\tfor (i, (x, y)) in enumerate(train_loader):\n",
    "\t\t# send the input to the device\n",
    "\t\t# perform a forward pass and calculate the training loss\n",
    "\t\tpred = seg(x)\n",
    "\t\tloss = lossFunc(pred, y)\n",
    "\t\t# first, zero out any previously accumulated gradients, then\n",
    "\t\t# perform backpropagation, and then update model parameters\n",
    "\t\topt.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "\t\t# add the loss to the total training loss so far\n",
    "\t\ttotalTrainLoss += loss\n",
    "\t# switch off autograd\n",
    "\twith torch.no_grad():\n",
    "\t\t# set the model in evaluation mode\n",
    "\t\tseg.eval()\n",
    "\t\t# loop over the validation set\n",
    "\t\tfor (x, y) in test_loader:\n",
    "\t\t\t# send the input to the device\n",
    "\t\t\t\n",
    "\t\t\t# make the predictions and calculate the validation loss\n",
    "\t\t\tpred = seg(x)\n",
    "\t\t\ttotalTestLoss += lossFunc(pred, y)\n",
    "\t# calculate the average training and validation loss\n",
    "\tavgTrainLoss = totalTrainLoss / train_steps\n",
    "\tavgTestLoss = totalTestLoss / test_steps\n",
    "\t# update our training history\n",
    "\tH[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "\tH[\"test_loss\"].append(avgTestLoss.cpu().detach().numpy())\n",
    "\t# print the model training and validation information\n",
    "\tprint(\"[INFO] EPOCH: {}/{}\".format(e + 1, NUM_EPOCHS))\n",
    "\tprint(\"Train loss: {:.6f}, Test loss: {:.4f}\".format(\n",
    "\t\tavgTrainLoss, avgTestLoss))\n",
    "# display the total time needed to perform the training\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
    "\tendTime - startTime))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
